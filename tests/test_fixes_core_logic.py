#!/usr/bin/env python3
"""
APIä¿®å¤æ ¸å¿ƒé€»è¾‘éªŒè¯
æµ‹è¯•ä¿®å¤çš„å…³é”®é€»è¾‘è€Œä¸ä¾èµ–å¤–éƒ¨åº“
"""

import sys
import json
from enum import Enum
from dataclasses import dataclass
from typing import Dict, Any, Optional

# æ¨¡æ‹Ÿæ ¸å¿ƒç±»å‹
class AIProvider(Enum):
    OPENAI = "openai"
    CLAUDE = "claude"
    GEMINI = "gemini"
    OLLAMA = "ollama"
    CUSTOM = "custom"

@dataclass
class AIConfig:
    provider: AIProvider
    model: str
    endpoint_url: Optional[str] = None
    max_tokens: int = 2000
    temperature: float = 0.8
    top_p: float = 0.9
    timeout: int = 30
    max_retries: int = 3
    disable_ssl_verify: bool = False
    reasoning_effort: str = "medium"
    _has_api_key: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'provider': self.provider.value,
            'model': self.model,
            'endpoint_url': self.endpoint_url,
            'max_tokens': self.max_tokens,
            'temperature': self.temperature,
            'top_p': self.top_p,
            'timeout': self.timeout,
            'max_retries': self.max_retries,
            'disable_ssl_verify': self.disable_ssl_verify,
            'reasoning_effort': self.reasoning_effort,
            'has_api_key': self._has_api_key
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AIConfig':
        config = cls(
            provider=AIProvider(data['provider']),
            model=data['model'],
            endpoint_url=data.get('endpoint_url'),
            max_tokens=data.get('max_tokens', 2000),
            temperature=data.get('temperature', 0.8),
            top_p=data.get('top_p', 0.9),
            timeout=data.get('timeout', 30),
            max_retries=data.get('max_retries', 3),
            disable_ssl_verify=data.get('disable_ssl_verify', False),
            reasoning_effort=data.get('reasoning_effort', 'medium')
        )
        return config


class CoreLogicTester:
    """æ ¸å¿ƒé€»è¾‘æµ‹è¯•å™¨"""
    
    def __init__(self, config: AIConfig):
        self.config = config
    
    def _is_reasoning_model(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯reasoning model"""
        reasoning_models = ['o1', 'o3', 'o4-mini', 'o1-mini', 'o1-preview', 'o3-mini', 'o4']
        model_name = self.config.model.lower()
        return any(rm in model_name for rm in reasoning_models)
    
    def _is_thinking_model(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æ”¯æŒæ€è€ƒæ¨¡å¼çš„Geminiæ¨¡å‹"""
        if self.config.provider != AIProvider.GEMINI:
            return False
        
        thinking_models = [
            'gemini-2.5-pro', 'gemini-2.5-flash', 'gemini-2.5-flash-lite',
            'gemini-2.0-flash-thinking'
        ]
        model_name = self.config.model.lower()
        return any(tm in model_name for tm in thinking_models)
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        headers = {'Content-Type': 'application/json'}
        
        if self.config.provider == AIProvider.OPENAI:
            headers['Authorization'] = 'Bearer test-key'
        elif self.config.provider == AIProvider.CLAUDE:
            headers['x-api-key'] = 'test-key'
        elif self.config.provider == AIProvider.GEMINI:
            headers['x-goog-api-key'] = 'test-key'
        elif self.config.provider == AIProvider.OLLAMA:
            pass  # ä¸éœ€è¦è®¤è¯
        
        return headers
    
    def build_request_data(self, messages, **kwargs) -> Dict[str, Any]:
        """æ„å»ºè¯·æ±‚æ•°æ®"""
        data = {"model": self.config.model, "stream": False}
        is_reasoning_model = self._is_reasoning_model()
        
        if self.config.provider == AIProvider.CLAUDE:
            data["messages"] = messages
            data["max_tokens"] = self.config.max_tokens
            data["temperature"] = self.config.temperature
            
        elif self.config.provider == AIProvider.GEMINI:
            # GeminiåŸç”Ÿæ ¼å¼
            contents = []
            generation_config = {
                "temperature": self.config.temperature,
                "maxOutputTokens": self.config.max_tokens,
                "topP": self.config.top_p
            }
            
            # æ€è€ƒæ¨¡å¼é…ç½®
            if self._is_thinking_model():
                thinking_config = {}
                
                include_thoughts = kwargs.get("include_thoughts", kwargs.get("includeThoughts", False))
                if include_thoughts:
                    thinking_config["includeThoughts"] = True
                
                thinking_budget = kwargs.get("thinking_budget", kwargs.get("thinkingBudget"))
                if thinking_budget is not None:
                    thinking_config["thinkingBudget"] = thinking_budget
                elif include_thoughts:
                    thinking_config["thinkingBudget"] = 1024
                
                if thinking_config:
                    generation_config["thinkingConfig"] = thinking_config
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            for msg in messages:
                gemini_role = "model" if msg["role"] == "assistant" else msg["role"]
                contents.append({
                    "role": gemini_role,
                    "parts": [{"text": msg["content"]}]
                })
            
            data = {
                "contents": contents,
                "generationConfig": generation_config
            }
            
        elif self.config.provider == AIProvider.OLLAMA:
            data["messages"] = messages
            data["max_tokens"] = self.config.max_tokens
            data["temperature"] = self.config.temperature
            data["top_p"] = self.config.top_p
            
            # Ollamaç‰¹æœ‰å‚æ•°
            ollama_options = {}
            for param in ["num_ctx", "num_predict", "repeat_penalty", "top_k", "seed"]:
                if param in kwargs:
                    ollama_options[param] = kwargs[param]
            
            if ollama_options:
                data["options"] = ollama_options
                
        elif is_reasoning_model:
            # æ¨ç†æ¨¡å‹æ ¼å¼
            data["messages"] = messages
            data["max_completion_tokens"] = self.config.max_tokens
            
            if "reasoning_effort" in kwargs:
                data["reasoning_effort"] = kwargs["reasoning_effort"]
            elif hasattr(self.config, 'reasoning_effort'):
                data["reasoning_effort"] = self.config.reasoning_effort
                
        else:
            # æ ‡å‡†OpenAIæ ¼å¼
            data["messages"] = messages
            data["max_tokens"] = self.config.max_tokens
            data["temperature"] = self.config.temperature
            data["top_p"] = self.config.top_p
        
        # å¤„ç†kwargså‚æ•°
        for key, value in kwargs.items():
            if self.config.provider == AIProvider.GEMINI and key == "max_tokens":
                if "generationConfig" in data:
                    data["generationConfig"]["maxOutputTokens"] = value
                continue
            elif key == "max_tokens" and is_reasoning_model:
                data["max_completion_tokens"] = value
            elif key not in ["max_tokens", "temperature", "top_p", "timeout", "max_retries", 
                           "disable_ssl_verify", "num_ctx", "num_predict", "repeat_penalty", 
                           "top_k", "seed", "include_thoughts", "includeThoughts", 
                           "thinking_budget", "thinkingBudget", "reasoning_effort"]:
                data[key] = value
        
        return data


def test_openai_reasoning_models():
    """æµ‹è¯•OpenAIæ¨ç†æ¨¡å‹ä¿®å¤"""
    print("ğŸ§ª æµ‹è¯•OpenAIæ¨ç†æ¨¡å‹...")
    
    # æµ‹è¯•å„ç§æ¨ç†æ¨¡å‹
    models = ["o1", "o1-mini", "o3", "o3-mini", "o4-mini"]
    
    for model in models:
        config = AIConfig(
            provider=AIProvider.OPENAI,
            model=model,
            reasoning_effort="high"
        )
        
        tester = CoreLogicTester(config)
        
        # éªŒè¯æ¨¡å‹è¯†åˆ«
        if not tester._is_reasoning_model():
            print(f"  âŒ {model} æœªè¢«è¯†åˆ«ä¸ºæ¨ç†æ¨¡å‹")
            return False
        
        # æµ‹è¯•è¯·æ±‚æ„å»º
        messages = [{"role": "user", "content": "Test"}]
        data = tester.build_request_data(
            messages, 
            max_tokens=500,
            temperature=0.5,  # åº”è¯¥è¢«å¿½ç•¥
            reasoning_effort="medium"
        )
        
        # éªŒè¯å‚æ•°
        if "max_completion_tokens" not in data:
            print(f"  âŒ {model} ç¼ºå°‘max_completion_tokens")
            return False
        
        if "temperature" in data:
            print(f"  âŒ {model} ä¸åº”è¯¥åŒ…å«temperature")
            return False
        
        if data.get("max_completion_tokens") != 500:
            print(f"  âŒ {model} max_completion_tokenså€¼é”™è¯¯")
            return False
        
        if data.get("reasoning_effort") != "medium":
            print(f"  âŒ {model} reasoning_effortå€¼é”™è¯¯")
            return False
    
    print("  âœ… OpenAIæ¨ç†æ¨¡å‹ä¿®å¤æ­£ç¡®")
    return True


def test_gemini_thinking_mode():
    """æµ‹è¯•Geminiæ€è€ƒæ¨¡å¼ä¿®å¤"""
    print("ğŸ§ª æµ‹è¯•Geminiæ€è€ƒæ¨¡å¼...")
    
    models = ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.0-flash-thinking"]
    
    for model in models:
        config = AIConfig(
            provider=AIProvider.GEMINI,
            model=model
        )
        
        tester = CoreLogicTester(config)
        
        # éªŒè¯æ€è€ƒæ¨¡å‹è¯†åˆ«
        if not tester._is_thinking_model():
            print(f"  âŒ {model} æœªè¢«è¯†åˆ«ä¸ºæ€è€ƒæ¨¡å‹")
            return False
        
        # æµ‹è¯•æ€è€ƒé…ç½®
        messages = [{"role": "user", "content": "Think about this"}]
        data = tester.build_request_data(
            messages,
            include_thoughts=True,
            thinking_budget=2048
        )
        
        # éªŒè¯æ ¼å¼
        if "generationConfig" not in data:
            print(f"  âŒ {model} ç¼ºå°‘generationConfig")
            return False
        
        gen_config = data["generationConfig"]
        if "thinkingConfig" not in gen_config:
            print(f"  âŒ {model} ç¼ºå°‘thinkingConfig")
            return False
        
        thinking_config = gen_config["thinkingConfig"]
        if not thinking_config.get("includeThoughts"):
            print(f"  âŒ {model} includeThoughtsæœªè®¾ç½®")
            return False
        
        if thinking_config.get("thinkingBudget") != 2048:
            print(f"  âŒ {model} thinkingBudgetå€¼é”™è¯¯")
            return False
    
    print("  âœ… Geminiæ€è€ƒæ¨¡å¼ä¿®å¤æ­£ç¡®")
    return True


def test_claude_format():
    """æµ‹è¯•Claudeæ ¼å¼"""
    print("ğŸ§ª æµ‹è¯•Claudeæ ¼å¼...")
    
    config = AIConfig(
        provider=AIProvider.CLAUDE,
        model="claude-3-5-sonnet-20241022"
    )
    
    tester = CoreLogicTester(config)
    
    messages = [{"role": "user", "content": "Test"}]
    data = tester.build_request_data(messages)
    
    # éªŒè¯Claudeæ ¼å¼ç‰¹å¾
    if "messages" not in data:
        print("  âŒ Claudeç¼ºå°‘messageså­—æ®µ")
        return False
    
    if "max_tokens" not in data:
        print("  âŒ Claudeç¼ºå°‘max_tokenså­—æ®µ")
        return False
    
    # æµ‹è¯•å·¥å…·æ ¼å¼ï¼ˆæ¨¡æ‹Ÿï¼‰
    tool_format = {
        "name": "get_weather",
        "description": "è·å–å¤©æ°”ä¿¡æ¯",
        "input_schema": {
            "type": "object",
            "properties": {"location": {"type": "string"}},
            "required": ["location"]
        }
    }
    
    if "input_schema" not in tool_format:
        print("  âŒ Claudeå·¥å…·æ ¼å¼ç¼ºå°‘input_schema")
        return False
    
    print("  âœ… Claudeæ ¼å¼æ­£ç¡®")
    return True


def test_ollama_integration():
    """æµ‹è¯•Ollamaé›†æˆä¿®å¤"""
    print("ğŸ§ª æµ‹è¯•Ollamaé›†æˆ...")
    
    config = AIConfig(
        provider=AIProvider.OLLAMA,
        model="llama3.1:8b"
    )
    
    tester = CoreLogicTester(config)
    
    # æµ‹è¯•å¤´éƒ¨ï¼ˆä¸åº”è¯¥æœ‰è®¤è¯ï¼‰
    headers = tester.get_headers()
    
    if "Authorization" in headers or "x-api-key" in headers:
        print("  âŒ Ollamaä¸åº”è¯¥æœ‰è®¤è¯å¤´")
        return False
    
    # æµ‹è¯•ç‰¹æœ‰å‚æ•°æ”¯æŒ
    messages = [{"role": "user", "content": "Test"}]
    data = tester.build_request_data(
        messages,
        num_ctx=4096,
        num_predict=512,
        repeat_penalty=1.1,
        top_k=40,
        seed=42
    )
    
    if "options" not in data:
        print("  âŒ Ollamaç¼ºå°‘optionså‚æ•°")
        return False
    
    options = data["options"]
    expected = {
        "num_ctx": 4096,
        "num_predict": 512,
        "repeat_penalty": 1.1,
        "top_k": 40,
        "seed": 42
    }
    
    for key, expected_value in expected.items():
        if options.get(key) != expected_value:
            print(f"  âŒ Ollama {key}å‚æ•°é”™è¯¯: æœŸæœ›{expected_value}, å®é™…{options.get(key)}")
            return False
    
    print("  âœ… Ollamaé›†æˆä¿®å¤æ­£ç¡®")
    return True


def test_config_serialization():
    """æµ‹è¯•é…ç½®åºåˆ—åŒ–æ”¯æŒæ–°å‚æ•°"""
    print("ğŸ§ª æµ‹è¯•é…ç½®åºåˆ—åŒ–...")
    
    config = AIConfig(
        provider=AIProvider.OPENAI,
        model="o3-mini",
        reasoning_effort="high",
        temperature=0.7,
        max_tokens=1000
    )
    
    # æµ‹è¯•åºåˆ—åŒ–
    config_dict = config.to_dict()
    
    if "reasoning_effort" not in config_dict:
        print("  âŒ åºåˆ—åŒ–ç¼ºå°‘reasoning_effortå‚æ•°")
        return False
    
    if config_dict["reasoning_effort"] != "high":
        print(f"  âŒ reasoning_effortå€¼é”™è¯¯: {config_dict['reasoning_effort']}")
        return False
    
    # æµ‹è¯•ååºåˆ—åŒ–
    restored_config = AIConfig.from_dict(config_dict)
    
    if restored_config.reasoning_effort != "high":
        print(f"  âŒ ååºåˆ—åŒ–reasoning_efforté”™è¯¯: {restored_config.reasoning_effort}")
        return False
    
    print("  âœ… é…ç½®åºåˆ—åŒ–ä¿®å¤æ­£ç¡®")
    return True


def test_parameter_exclusion():
    """æµ‹è¯•å‚æ•°æ’é™¤é€»è¾‘"""
    print("ğŸ§ª æµ‹è¯•å‚æ•°æ’é™¤é€»è¾‘...")
    
    # æµ‹è¯•å„ç§æä¾›å•†çš„å‚æ•°æ’é™¤
    test_cases = [
        (AIProvider.OPENAI, "gpt-4"),
        (AIProvider.CLAUDE, "claude-3-5-sonnet-20241022"),
        (AIProvider.GEMINI, "gemini-1.5-pro"),
        (AIProvider.OLLAMA, "llama3.1:8b"),
    ]
    
    for provider, model in test_cases:
        config = AIConfig(provider=provider, model=model)
        tester = CoreLogicTester(config)
        
        messages = [{"role": "user", "content": "Test"}]
        data = tester.build_request_data(
            messages,
            timeout=60,  # åº”è¯¥è¢«æ’é™¤
            max_retries=5,  # åº”è¯¥è¢«æ’é™¤
            disable_ssl_verify=True,  # åº”è¯¥è¢«æ’é™¤
            custom_param="test"  # åº”è¯¥è¢«ä¿ç•™
        )
        
        # éªŒè¯å±é™©å‚æ•°è¢«æ’é™¤
        excluded_params = ["timeout", "max_retries", "disable_ssl_verify"]
        for param in excluded_params:
            if param in data:
                print(f"  âŒ {provider.value} {param}å‚æ•°åº”è¯¥è¢«æ’é™¤")
                return False
        
        # éªŒè¯è‡ªå®šä¹‰å‚æ•°è¢«ä¿ç•™ï¼ˆé™¤äº†Geminiçš„ç‰¹æ®Šæƒ…å†µï¼‰
        if provider != AIProvider.GEMINI and "custom_param" not in data:
            print(f"  âŒ {provider.value} custom_paramåº”è¯¥è¢«ä¿ç•™")
            return False
    
    print("  âœ… å‚æ•°æ’é™¤é€»è¾‘æ­£ç¡®")
    return True


def run_core_logic_tests():
    """è¿è¡Œæ ¸å¿ƒé€»è¾‘æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ ¸å¿ƒé€»è¾‘éªŒè¯æµ‹è¯•...\n")
    
    tests = [
        test_openai_reasoning_models,
        test_gemini_thinking_mode,
        test_claude_format,
        test_ollama_integration,
        test_config_serialization,
        test_parameter_exclusion
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"âŒ {test.__name__} å¼‚å¸¸: {e}")
            failed += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"âœ… é€šè¿‡: {passed}")
    print(f"âŒ å¤±è´¥: {failed}")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒé€»è¾‘æµ‹è¯•é€šè¿‡!")
        print("\nğŸ“‹ 2025å¹´APIä¿®å¤éªŒè¯æˆåŠŸ:")
        print("  âœ¨ OpenAIæ¨ç†æ¨¡å‹å‚æ•°æ”¯æŒå®Œæ•´")
        print("  âœ¨ Geminiæ€è€ƒæ¨¡å¼é…ç½®æ ¼å¼æ­£ç¡®")
        print("  âœ¨ Claudeå·¥å…·è°ƒç”¨æ ¼å¼ç¬¦åˆè§„èŒƒ")
        print("  âœ¨ Ollamaé›†æˆä¼˜åŒ–æ— éœ€è®¤è¯")
        print("  âœ¨ é…ç½®åºåˆ—åŒ–å…¼å®¹æ–°å‚æ•°")
        print("  âœ¨ å‚æ•°è¿‡æ»¤é€»è¾‘å®‰å…¨å¯é ")
        
        print(f"\nğŸ”§ ä¿®å¤è¦ç‚¹:")
        print(f"  â€¢ OpenAIæ¨ç†æ¨¡å‹ä½¿ç”¨max_completion_tokensè€Œémax_tokens")
        print(f"  â€¢ Geminiæ€è€ƒé…ç½®åœ¨generationConfig.thinkingConfigä¸­")
        print(f"  â€¢ Claudeå·¥å…·ä½¿ç”¨input_schemaè€Œéparameters")
        print(f"  â€¢ Ollamaæ— éœ€APIå¯†é’¥ä¸”æ”¯æŒç‰¹æœ‰å‚æ•°")
        print(f"  â€¢ å±é™©å‚æ•°è¢«æ­£ç¡®æ’é™¤ï¼Œé¿å…APIé”™è¯¯")
        
        return True
    else:
        print(f"\nâš ï¸ {failed}ä¸ªæµ‹è¯•å¤±è´¥ï¼Œæ ¸å¿ƒé€»è¾‘éœ€è¦æ£€æŸ¥")
        return False


if __name__ == "__main__":
    success = run_core_logic_tests()
    sys.exit(0 if success else 1)