#!/usr/bin/env python3
"""
è°ƒè¯•max_tokensé…ç½®é—®é¢˜
æ£€æŸ¥æ•´ä¸ªé…ç½®ä¼ é€’é“¾æ¡
"""

import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from core.config import Config
    from core.ai_client import AIConfig, AIProvider
    print("âœ… æˆåŠŸå¯¼å…¥é…ç½®æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)


def debug_config_chain():
    """è°ƒè¯•é…ç½®ä¼ é€’é“¾æ¡"""
    print("ğŸ” è°ƒè¯•é…ç½®ä¼ é€’é“¾æ¡...\n")
    
    # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶
    try:
        config = Config()
        ai_section = config.get_section('ai')
        
        print("ğŸ“ é…ç½®æ–‡ä»¶ä¸­çš„AIè®¾ç½®:")
        print(f"  â€¢ provider: {ai_section.get('provider', 'N/A')}")
        print(f"  â€¢ model: {ai_section.get('model', 'N/A')}")
        print(f"  â€¢ max_tokens: {ai_section.get('max_tokens', 'N/A')} (ç±»å‹: {type(ai_section.get('max_tokens'))})")
        print(f"  â€¢ temperature: {ai_section.get('temperature', 'N/A')}")
        print(f"  â€¢ top_p: {ai_section.get('top_p', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    print()
    
    # 2. æ£€æŸ¥get_ai_configæ–¹æ³•
    try:
        ai_config = config.get_ai_config()
        
        if ai_config:
            print("ğŸ”§ get_ai_configè¿”å›çš„é…ç½®:")
            print(f"  â€¢ provider: {ai_config.provider}")
            print(f"  â€¢ model: {ai_config.model}")
            print(f"  â€¢ max_tokens: {ai_config.max_tokens} (ç±»å‹: {type(ai_config.max_tokens)})")
            print(f"  â€¢ temperature: {ai_config.temperature}")
            print(f"  â€¢ top_p: {ai_config.top_p}")
            print(f"  â€¢ reasoning_effort: {ai_config.reasoning_effort}")
        else:
            print("âŒ get_ai_configè¿”å›None")
            return False
            
    except Exception as e:
        print(f"âŒ get_ai_configå¤±è´¥: {e}")
        return False
    
    print()
    
    # 3. æ¨¡æ‹Ÿenhanced_ai_managerçš„_get_max_tokensæ–¹æ³•
    try:
        print("ğŸ¯ æ¨¡æ‹Ÿenhanced_ai_managerçš„_get_max_tokens:")
        
        # æ¨¡æ‹Ÿ_get_max_tokensé€»è¾‘
        def simulate_get_max_tokens(context_mode: str) -> int:
            try:
                # ä»AIé…ç½®è·å–ç”¨æˆ·è®¾ç½®çš„max_tokens
                ai_config = config.get_ai_config()
                if ai_config and hasattr(ai_config, 'max_tokens'):
                    base_tokens = ai_config.max_tokens
                else:
                    # å›é€€åˆ°é…ç½®æ–‡ä»¶ä¸­çš„å€¼
                    ai_section = config.get_section('ai')
                    base_tokens = ai_section.get('max_tokens', 2000)
            except Exception as e:
                print(f"  è­¦å‘Š: è·å–max_tokensé…ç½®å¤±è´¥: {e}")
                base_tokens = 2000  # åˆç†çš„é»˜è®¤å€¼
            
            # æ ¹æ®ä¸Šä¸‹æ–‡æ¨¡å¼è°ƒæ•´tokenæ•°é‡ï¼ˆä½¿ç”¨æ¯”ä¾‹è€Œéå›ºå®šå€¼ï¼‰
            mode_multipliers = {
                "fast": 0.4,      # 40% - å¿«é€Ÿæ¨¡å¼
                "balanced": 0.6,  # 60% - å¹³è¡¡æ¨¡å¼  
                "full": 1.0       # 100% - å…¨å±€æ¨¡å¼
            }
            
            multiplier = mode_multipliers.get(context_mode, 0.6)
            adjusted_tokens = int(base_tokens * multiplier)
            
            # ç¡®ä¿æœ‰åˆç†çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
            min_tokens = 50
            max_tokens = 8000
            
            result = max(min_tokens, min(adjusted_tokens, max_tokens))
            print(f"  ğŸ“Š Tokenè®¡ç®—: base={base_tokens}, mode={context_mode}, multiplier={multiplier}, result={result}")
            
            return result
        
        # æµ‹è¯•å„ç§æ¨¡å¼
        for mode in ["fast", "balanced", "full"]:
            tokens = simulate_get_max_tokens(mode)
            print(f"  â€¢ {mode}æ¨¡å¼: {tokens} tokens")
            
    except Exception as e:
        print(f"âŒ æ¨¡æ‹Ÿ_get_max_tokenså¤±è´¥: {e}")
        return False
    
    print()
    
    # 4. æ£€æŸ¥Geminiè¯·æ±‚æ„å»º
    try:
        if ai_config and ai_config.provider == AIProvider.GEMINI:
            print("ğŸ”® Geminiè¯·æ±‚æ„å»ºæµ‹è¯•:")
            
            # æ¨¡æ‹Ÿè¯·æ±‚æ„å»º
            generation_config = {
                "temperature": ai_config.temperature,
                "maxOutputTokens": ai_config.max_tokens,
                "topP": ai_config.top_p
            }
            
            print(f"  â€¢ åˆå§‹generationConfig:")
            print(f"    - maxOutputTokens: {generation_config['maxOutputTokens']}")
            print(f"    - temperature: {generation_config['temperature']}")
            print(f"    - topP: {generation_config['topP']}")
            
            # æ¨¡æ‹Ÿkwargsè¦†ç›–
            kwargs_max_tokens = simulate_get_max_tokens("balanced")
            generation_config["maxOutputTokens"] = kwargs_max_tokens
            
            print(f"  â€¢ kwargsè¦†ç›–å:")
            print(f"    - maxOutputTokens: {generation_config['maxOutputTokens']} (æ¥è‡ª_get_max_tokens)")
            
        else:
            print("â„¹ï¸ å½“å‰é…ç½®ä¸æ˜¯Geminiï¼Œè·³è¿‡Geminiæµ‹è¯•")
            
    except Exception as e:
        print(f"âŒ Geminiè¯·æ±‚æ„å»ºæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print()
    
    # 5. æ£€æŸ¥å¯èƒ½çš„é—®é¢˜ç‚¹
    print("âš ï¸ å¯èƒ½çš„é—®é¢˜ç‚¹:")
    
    # æ£€æŸ¥é…ç½®ç±»å‹
    max_tokens_raw = ai_section.get('max_tokens')
    if isinstance(max_tokens_raw, str):
        print(f"  â€¢ max_tokensåœ¨é…ç½®æ–‡ä»¶ä¸­æ˜¯å­—ç¬¦ä¸²: '{max_tokens_raw}' - å¯èƒ½éœ€è¦ç±»å‹è½¬æ¢")
    
    # æ£€æŸ¥é…ç½®å€¼
    if ai_config and ai_config.max_tokens < 1000:
        print(f"  â€¢ max_tokenså€¼è¾ƒå°: {ai_config.max_tokens} - å¯èƒ½å¯¼è‡´æˆªæ–­")
    
    # æ£€æŸ¥provideråŒ¹é…
    if ai_config and ai_config.provider != AIProvider.GEMINI:
        print(f"  â€¢ å½“å‰provideræ˜¯{ai_config.provider.value}ï¼Œä¸æ˜¯Gemini")
    
    return True


def test_config_save_load():
    """æµ‹è¯•é…ç½®ä¿å­˜å’ŒåŠ è½½"""
    print("ğŸ’¾ æµ‹è¯•é…ç½®ä¿å­˜å’ŒåŠ è½½...\n")
    
    try:
        config = Config()
        
        # ä¿å­˜æµ‹è¯•å€¼
        test_max_tokens = 3000
        config.set('ai', 'max_tokens', test_max_tokens)
        print(f"ğŸ“ ä¿å­˜max_tokens: {test_max_tokens}")
        
        # é‡æ–°è¯»å–
        ai_section = config.get_section('ai')
        loaded_value = ai_section.get('max_tokens')
        print(f"ğŸ“– è¯»å–max_tokens: {loaded_value} (ç±»å‹: {type(loaded_value)})")
        
        if loaded_value == test_max_tokens:
            print("âœ… é…ç½®ä¿å­˜å’ŒåŠ è½½æ­£å¸¸")
        else:
            print(f"âŒ é…ç½®ä¿å­˜å’ŒåŠ è½½å¼‚å¸¸: æœŸæœ›{test_max_tokens}, å®é™…{loaded_value}")
            
        # æµ‹è¯•get_ai_config
        ai_config = config.get_ai_config()
        if ai_config:
            print(f"ğŸ”§ AIConfigä¸­çš„max_tokens: {ai_config.max_tokens} (ç±»å‹: {type(ai_config.max_tokens)})")
            
            if ai_config.max_tokens == test_max_tokens:
                print("âœ… AIConfigè¯»å–æ­£ç¡®")
            else:
                print(f"âŒ AIConfigè¯»å–é”™è¯¯: æœŸæœ›{test_max_tokens}, å®é™…{ai_config.max_tokens}")
        
    except Exception as e:
        print(f"âŒ é…ç½®ä¿å­˜åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è°ƒè¯•max_tokensé…ç½®é—®é¢˜...\n")
    
    success1 = debug_config_chain()
    print("="*60)
    success2 = test_config_save_load()
    
    if success1 and success2:
        print("\nğŸ‰ é…ç½®è°ƒè¯•å®Œæˆï¼Œè¯·æ£€æŸ¥ä¸Šè¿°è¾“å‡ºæ‰¾å‡ºé—®é¢˜åŸå› ")
    else:
        print("\nâš ï¸ å‘ç°é…ç½®é—®é¢˜ï¼Œè¯·æ ¹æ®ä¸Šè¿°è¾“å‡ºè¿›è¡Œä¿®å¤")