#!/usr/bin/env python3
"""
2025å¹´APIä¿®å¤ç»¼åˆéªŒè¯æµ‹è¯•
éªŒè¯æ‰€æœ‰æœ€æ–°APIæ ¼å¼å’Œå‚æ•°ä¿®å¤
"""

import sys
from pathlib import Path
import json

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from core.ai_client import AIConfig, AIProvider, AIClient
    from core.tool_types import ToolDefinition, ToolParameter
    print("âœ… æˆåŠŸå¯¼å…¥AIå®¢æˆ·ç«¯æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)


def test_openai_reasoning_models():
    """æµ‹è¯•OpenAIæ¨ç†æ¨¡å‹å‚æ•°ä¿®å¤"""
    print("\nğŸ§ª æµ‹è¯•OpenAIæ¨ç†æ¨¡å‹ä¿®å¤...")
    
    # æµ‹è¯•å„ç§æ¨ç†æ¨¡å‹
    reasoning_models = ["o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o4-mini"]
    
    for model in reasoning_models:
        print(f"\n  ğŸ” æµ‹è¯•æ¨¡å‹: {model}")
        
        config = AIConfig(
            provider=AIProvider.OPENAI,
            model=model,
            temperature=0.8,  # è¿™ä¸ªåº”è¯¥è¢«å¿½ç•¥
            max_tokens=1000,
            reasoning_effort="high"
        )
        
        client = AIClient(config)
        
        # éªŒè¯æ¨¡å‹è¯†åˆ«
        if not client._is_reasoning_model():
            print(f"    âŒ {model} æ²¡æœ‰è¢«è¯†åˆ«ä¸ºæ¨ç†æ¨¡å‹")
            return False
        else:
            print(f"    âœ… {model} æ­£ç¡®è¯†åˆ«ä¸ºæ¨ç†æ¨¡å‹")
        
        # æµ‹è¯•è¯·æ±‚æ„å»º
        messages = [{"role": "user", "content": "Test"}]
        data = client._build_request_data(
            messages, 
            stream=False,
            max_tokens=500,  # åº”è¯¥è½¬æ¢ä¸ºmax_completion_tokens
            temperature=0.5,  # åº”è¯¥è¢«å¿½ç•¥
            reasoning_effort="medium"
        )
        
        # éªŒè¯å‚æ•°
        if "max_completion_tokens" not in data:
            print(f"    âŒ ç¼ºå°‘max_completion_tokenså‚æ•°")
            return False
        
        if "temperature" in data:
            print(f"    âŒ ä¸åº”è¯¥åŒ…å«temperatureå‚æ•°")
            return False
        
        if data.get("max_completion_tokens") != 500:
            print(f"    âŒ max_completion_tokenså€¼é”™è¯¯: {data.get('max_completion_tokens')}")
            return False
        
        if data.get("reasoning_effort") != "medium":
            print(f"    âŒ reasoning_effortå€¼é”™è¯¯: {data.get('reasoning_effort')}")
            return False
        
        print(f"    âœ… {model} å‚æ•°æ ¼å¼æ­£ç¡®")
    
    return True


def test_gemini_thinking_mode():
    """æµ‹è¯•Geminiæ€è€ƒæ¨¡å¼é…ç½®ä¿®å¤"""
    print("\nğŸ§ª æµ‹è¯•Geminiæ€è€ƒæ¨¡å¼ä¿®å¤...")
    
    thinking_models = ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.0-flash-thinking"]
    
    for model in thinking_models:
        print(f"\n  ğŸ” æµ‹è¯•æ¨¡å‹: {model}")
        
        config = AIConfig(
            provider=AIProvider.GEMINI,
            model=model,
            temperature=0.7,
            max_tokens=1000
        )
        
        client = AIClient(config)
        
        # éªŒè¯æ€è€ƒæ¨¡å‹è¯†åˆ«
        if not client._is_thinking_model():
            print(f"    âŒ {model} æ²¡æœ‰è¢«è¯†åˆ«ä¸ºæ€è€ƒæ¨¡å‹")
            return False
        else:
            print(f"    âœ… {model} æ­£ç¡®è¯†åˆ«ä¸ºæ€è€ƒæ¨¡å‹")
        
        # æµ‹è¯•æ€è€ƒé…ç½®
        messages = [{"role": "user", "content": "Think about this problem"}]
        data = client._build_request_data(
            messages,
            stream=False,
            include_thoughts=True,
            thinking_budget=2048
        )
        
        # éªŒè¯æ ¼å¼
        if "generationConfig" not in data:
            print(f"    âŒ ç¼ºå°‘generationConfig")
            return False
        
        gen_config = data["generationConfig"]
        
        if "thinkingConfig" not in gen_config:
            print(f"    âŒ ç¼ºå°‘thinkingConfig")
            return False
        
        thinking_config = gen_config["thinkingConfig"]
        
        if not thinking_config.get("includeThoughts"):
            print(f"    âŒ includeThoughtsæœªæ­£ç¡®è®¾ç½®")
            return False
        
        if thinking_config.get("thinkingBudget") != 2048:
            print(f"    âŒ thinkingBudgetå€¼é”™è¯¯: {thinking_config.get('thinkingBudget')}")
            return False
        
        print(f"    âœ… {model} æ€è€ƒé…ç½®æ ¼å¼æ­£ç¡®")
    
    return True


def test_claude_tool_calling():
    """æµ‹è¯•Claudeå·¥å…·è°ƒç”¨æ ¼å¼"""
    print("\nğŸ§ª æµ‹è¯•Claudeå·¥å…·è°ƒç”¨æ ¼å¼...")
    
    config = AIConfig(
        provider=AIProvider.CLAUDE,
        model="claude-3-5-sonnet-20241022",
        max_tokens=1000
    )
    
    client = AIClient(config)
    
    # åˆ›å»ºæµ‹è¯•å·¥å…·
    tool_param = ToolParameter(
        name="location",
        param_type="string",
        description="åŸå¸‚åç§°",
        required=True
    )
    
    tool = ToolDefinition(
        name="get_weather",
        description="è·å–å¤©æ°”ä¿¡æ¯",
        parameters=[tool_param],
        function=lambda x: f"Weather in {x}"
    )
    
    # æµ‹è¯•Claudeæ ¼å¼è½¬æ¢
    claude_format = tool.to_claude_format()
    
    print(f"  ğŸ“‹ Claudeå·¥å…·æ ¼å¼: {json.dumps(claude_format, indent=2, ensure_ascii=False)}")
    
    # éªŒè¯æ ¼å¼
    if "input_schema" not in claude_format:
        print(f"    âŒ ç¼ºå°‘input_schemaå­—æ®µ")
        return False
    
    if "parameters" in claude_format:
        print(f"    âŒ ä¸åº”è¯¥åŒ…å«parameterså­—æ®µï¼ˆåº”è¯¥æ˜¯input_schemaï¼‰")
        return False
    
    print(f"    âœ… Claudeå·¥å…·è°ƒç”¨æ ¼å¼æ­£ç¡®")
    
    return True


def test_ollama_integration():
    """æµ‹è¯•Ollamaé›†æˆä¿®å¤"""
    print("\nğŸ§ª æµ‹è¯•Ollamaé›†æˆä¿®å¤...")
    
    config = AIConfig(
        provider=AIProvider.OLLAMA,
        model="llama3.1:8b",
        temperature=0.8,
        max_tokens=1000
    )
    
    client = AIClient(config)
    
    # æµ‹è¯•å¤´éƒ¨ç”Ÿæˆï¼ˆä¸åº”è¯¥åŒ…å«Authorizationï¼‰
    headers = client._get_headers()
    
    if "Authorization" in headers or "x-api-key" in headers:
        print(f"    âŒ Ollamaå¤´éƒ¨ä¸åº”è¯¥åŒ…å«è®¤è¯ä¿¡æ¯")
        return False
    
    print(f"    âœ… Ollamaå¤´éƒ¨æ ¼å¼æ­£ç¡®ï¼ˆæ— è®¤è¯ï¼‰")
    
    # æµ‹è¯•ç‰¹æœ‰å‚æ•°æ”¯æŒ
    messages = [{"role": "user", "content": "Test"}]
    data = client._build_request_data(
        messages,
        stream=False,
        num_ctx=4096,
        num_predict=512,
        repeat_penalty=1.1,
        top_k=40,
        seed=42
    )
    
    # éªŒè¯Ollamaç‰¹æœ‰å‚æ•°
    if "options" not in data:
        print(f"    âŒ ç¼ºå°‘optionså‚æ•°")
        return False
    
    options = data["options"]
    expected_options = {
        "num_ctx": 4096,
        "num_predict": 512,
        "repeat_penalty": 1.1,
        "top_k": 40,
        "seed": 42
    }
    
    for key, expected_value in expected_options.items():
        if options.get(key) != expected_value:
            print(f"    âŒ {key}å‚æ•°é”™è¯¯: æœŸæœ›{expected_value}, å®é™…{options.get(key)}")
            return False
    
    print(f"    âœ… Ollamaç‰¹æœ‰å‚æ•°æ”¯æŒæ­£ç¡®")
    
    return True


def test_parameter_exclusion():
    """æµ‹è¯•å‚æ•°æ’é™¤é€»è¾‘"""
    print("\nğŸ§ª æµ‹è¯•å‚æ•°æ’é™¤é€»è¾‘...")
    
    # æµ‹è¯•å„ç§æä¾›å•†çš„å‚æ•°æ’é™¤
    test_cases = [
        (AIProvider.OPENAI, "gpt-4"),
        (AIProvider.CLAUDE, "claude-3-5-sonnet-20241022"),
        (AIProvider.GEMINI, "gemini-1.5-pro"),
        (AIProvider.OLLAMA, "llama3.1:8b"),
    ]
    
    for provider, model in test_cases:
        print(f"\n  ğŸ” æµ‹è¯•æä¾›å•†: {provider.value}")
        
        config = AIConfig(
            provider=provider,
            model=model,
            timeout=30,
            max_retries=3,
            disable_ssl_verify=False
        )
        
        client = AIClient(config)
        messages = [{"role": "user", "content": "Test"}]
        
        data = client._build_request_data(
            messages,
            stream=False,
            timeout=60,  # åº”è¯¥è¢«æ’é™¤
            max_retries=5,  # åº”è¯¥è¢«æ’é™¤
            disable_ssl_verify=True,  # åº”è¯¥è¢«æ’é™¤
            custom_param="test"  # åº”è¯¥è¢«ä¿ç•™
        )
        
        # éªŒè¯å±é™©å‚æ•°è¢«æ’é™¤
        excluded_params = ["timeout", "max_retries", "disable_ssl_verify"]
        for param in excluded_params:
            if param in data:
                print(f"    âŒ {param}å‚æ•°åº”è¯¥è¢«æ’é™¤ä½†ä»åœ¨è¯·æ±‚ä¸­")
                return False
        
        # éªŒè¯è‡ªå®šä¹‰å‚æ•°è¢«ä¿ç•™
        if provider != AIProvider.GEMINI and "custom_param" not in data:
            print(f"    âŒ custom_paramåº”è¯¥è¢«ä¿ç•™")
            return False
        
        print(f"    âœ… {provider.value} å‚æ•°æ’é™¤é€»è¾‘æ­£ç¡®")
    
    return True


def test_config_serialization():
    """æµ‹è¯•é…ç½®åºåˆ—åŒ–æ”¯æŒæ–°å‚æ•°"""
    print("\nğŸ§ª æµ‹è¯•é…ç½®åºåˆ—åŒ–...")
    
    # åˆ›å»ºåŒ…å«æ–°å‚æ•°çš„é…ç½®
    config = AIConfig(
        provider=AIProvider.OPENAI,
        model="o3-mini",
        reasoning_effort="high",
        temperature=0.7,
        max_tokens=1000
    )
    
    # æµ‹è¯•åºåˆ—åŒ–
    config_dict = config.to_dict()
    
    if "reasoning_effort" not in config_dict:
        print(f"    âŒ åºåˆ—åŒ–ç¼ºå°‘reasoning_effortå‚æ•°")
        return False
    
    if config_dict["reasoning_effort"] != "high":
        print(f"    âŒ reasoning_effortå€¼é”™è¯¯")
        return False
    
    # æµ‹è¯•ååºåˆ—åŒ–
    restored_config = AIConfig.from_dict(config_dict)
    
    if restored_config.reasoning_effort != "high":
        print(f"    âŒ ååºåˆ—åŒ–reasoning_efforté”™è¯¯")
        return False
    
    print(f"    âœ… é…ç½®åºåˆ—åŒ–æ”¯æŒæ­£ç¡®")
    
    return True


def run_comprehensive_tests():
    """è¿è¡Œæ‰€æœ‰ç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹2025å¹´APIä¿®å¤ç»¼åˆéªŒè¯...")
    
    tests = [
        test_openai_reasoning_models,
        test_gemini_thinking_mode,
        test_claude_tool_calling,
        test_ollama_integration,
        test_parameter_exclusion,
        test_config_serialization
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
                print(f"âœ… {test.__name__} é€šè¿‡")
            else:
                failed += 1
                print(f"âŒ {test.__name__} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test.__name__} å¼‚å¸¸: {e}")
            failed += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"âœ… é€šè¿‡: {passed}")
    print(f"âŒ å¤±è´¥: {failed}")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("ğŸ”§ ä¿®å¤å†…å®¹æ€»ç»“:")
        print("  âœ¨ OpenAIæ¨ç†æ¨¡å‹å‚æ•°æ”¯æŒ (max_completion_tokens, reasoning_effort)")
        print("  âœ¨ Geminiæ€è€ƒæ¨¡å¼é…ç½® (thinkingConfigæ ¼å¼)")
        print("  âœ¨ Claudeå·¥å…·è°ƒç”¨æ ¼å¼ (input_schema)")
        print("  âœ¨ Ollamaé›†æˆä¼˜åŒ– (æ— éœ€APIå¯†é’¥, ç‰¹æœ‰å‚æ•°)")
        print("  âœ¨ å‚æ•°æ’é™¤é€»è¾‘å®Œå–„")
        print("  âœ¨ é…ç½®åºåˆ—åŒ–å…¼å®¹æ€§")
        
        print("\nğŸ“‹ æ”¯æŒçš„APIç‰¹æ€§:")
        print("  ğŸ”¹ OpenAI: o1/o3/o4ç³»åˆ—æ¨ç†æ¨¡å‹, developerè§’è‰², reasoning_effort")
        print("  ğŸ”¹ Claude: æœ€æ–°å·¥å…·è°ƒç”¨æ ¼å¼, systemå‚æ•°, input_schema")
        print("  ğŸ”¹ Gemini: æ€è€ƒæ¨¡å¼, thinkingConfig, åŸç”ŸAPIæ ¼å¼")
        print("  ğŸ”¹ Ollama: æœ¬åœ°éƒ¨ç½², ç‰¹æœ‰å‚æ•°, æ— è®¤è¯éœ€æ±‚")
        
        return True
    else:
        print(f"\nâš ï¸ {failed}ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        return False


if __name__ == "__main__":
    success = run_comprehensive_tests()
    sys.exit(0 if success else 1)