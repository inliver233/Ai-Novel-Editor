#!/usr/bin/env python3
"""
æµ‹è¯•GeminiåŸç”ŸAPIä¿®å¤
éªŒè¯max_tokensç­‰å‚æ•°çš„æ­£ç¡®å¤„ç†
"""

import json
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from core.ai_client import AIConfig, AIProvider, AIClient
    print("âœ… æˆåŠŸå¯¼å…¥AIå®¢æˆ·ç«¯æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)


def test_gemini_request_format():
    """æµ‹è¯•Geminiè¯·æ±‚æ ¼å¼çš„æ­£ç¡®æ€§"""
    print("\nğŸ§ª æµ‹è¯•Geminiè¯·æ±‚æ ¼å¼...")
    
    config = AIConfig(
        provider=AIProvider.GEMINI,
        model="gemini-1.5-pro",
        temperature=0.8,
        max_tokens=1000,
        top_p=0.9
    )
    
    client = AIClient(config)
    messages = [{"role": "user", "content": "Hello"}]
    
    # æµ‹è¯•ä¸ä¼ å…¥kwargsçš„æƒ…å†µ
    request_data = client._build_request_data(messages, stream=False)
    
    print("  ğŸ“‹ åŸºç¡€è¯·æ±‚æ ¼å¼:")
    print(f"    âœ… åŒ…å«contents: {'contents' in request_data}")
    print(f"    âœ… åŒ…å«generationConfig: {'generationConfig' in request_data}")
    print(f"    âœ… ä¸åŒ…å«max_tokens: {'max_tokens' not in request_data}")
    print(f"    âœ… ä¸åŒ…å«model: {'model' not in request_data}")
    
    gen_config = request_data.get("generationConfig", {})
    print(f"    âœ… maxOutputTokens: {gen_config.get('maxOutputTokens')}")
    print(f"    âœ… temperature: {gen_config.get('temperature')}")
    print(f"    âœ… topP: {gen_config.get('topP')}")
    
    return "max_tokens" not in request_data and "contents" in request_data


def test_gemini_kwargs_handling():
    """æµ‹è¯•Gemini kwargså‚æ•°å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•Gemini kwargså‚æ•°å¤„ç†...")
    
    config = AIConfig(
        provider=AIProvider.GEMINI,
        model="gemini-2.0-flash-exp",
        temperature=0.5,
        max_tokens=500
    )
    
    client = AIClient(config)
    messages = [{"role": "user", "content": "Test message"}]
    
    # æ¨¡æ‹Ÿæµ‹è¯•ä»£ç çš„è°ƒç”¨æ–¹å¼
    request_data = client._build_request_data(
        messages, 
        stream=False,
        max_tokens=50,  # è¿™ä¸ªåº”è¯¥è¦†ç›–configä¸­çš„å€¼
        temperature=0.3,  # è¿™ä¸ªåº”è¯¥è¦†ç›–configä¸­çš„å€¼
        top_p=0.8  # è¿™ä¸ªåº”è¯¥è¦†ç›–configä¸­çš„å€¼
    )
    
    print("  ğŸ“‹ kwargsè¦†ç›–æµ‹è¯•:")
    print(f"    âœ… ä¸åŒ…å«æ ¹çº§max_tokens: {'max_tokens' not in request_data}")
    print(f"    âœ… ä¸åŒ…å«æ ¹çº§temperature: {'temperature' not in request_data}")
    print(f"    âœ… ä¸åŒ…å«æ ¹çº§top_p: {'top_p' not in request_data}")
    
    gen_config = request_data.get("generationConfig", {})
    print(f"    âœ… maxOutputTokensè¢«kwargsè¦†ç›–: {gen_config.get('maxOutputTokens')} (åº”è¯¥æ˜¯50)")
    print(f"    âœ… temperatureè¢«kwargsè¦†ç›–: {gen_config.get('temperature')} (åº”è¯¥æ˜¯0.3)")
    print(f"    âœ… topPè¢«kwargsè¦†ç›–: {gen_config.get('topP')} (åº”è¯¥æ˜¯0.8)")
    
    # éªŒè¯å€¼æ˜¯å¦æ­£ç¡®
    success = (
        "max_tokens" not in request_data and
        gen_config.get("maxOutputTokens") == 50 and
        gen_config.get("temperature") == 0.3 and
        gen_config.get("topP") == 0.8
    )
    
    return success


def test_gemini_vs_openai_format():
    """å¯¹æ¯”Geminiå’ŒOpenAIæ ¼å¼çš„å·®å¼‚"""
    print("\nğŸ§ª å¯¹æ¯”Geminiå’ŒOpenAIæ ¼å¼...")
    
    # OpenAIæ ¼å¼
    openai_config = AIConfig(
        provider=AIProvider.OPENAI,
        model="gpt-4",
        temperature=0.7,
        max_tokens=100
    )
    
    openai_client = AIClient(openai_config)
    messages = [{"role": "user", "content": "Test"}]
    openai_data = openai_client._build_request_data(
        messages, stream=False, max_tokens=50
    )
    
    # Geminiæ ¼å¼
    gemini_config = AIConfig(
        provider=AIProvider.GEMINI,
        model="gemini-1.5-pro",
        temperature=0.7,
        max_tokens=100
    )
    
    gemini_client = AIClient(gemini_config)
    gemini_data = gemini_client._build_request_data(
        messages, stream=False, max_tokens=50
    )
    
    print("  ğŸ“‹ OpenAIæ ¼å¼ç‰¹å¾:")
    print(f"    â€¢ åŒ…å«model: {'model' in openai_data}")
    print(f"    â€¢ åŒ…å«messages: {'messages' in openai_data}")
    print(f"    â€¢ åŒ…å«max_tokens: {'max_tokens' in openai_data}")
    print(f"    â€¢ åŒ…å«temperature: {'temperature' in openai_data}")
    
    print("  ğŸ“‹ Geminiæ ¼å¼ç‰¹å¾:")
    print(f"    â€¢ åŒ…å«contents: {'contents' in gemini_data}")
    print(f"    â€¢ åŒ…å«generationConfig: {'generationConfig' in gemini_data}")
    print(f"    â€¢ ä¸åŒ…å«model: {'model' not in gemini_data}")
    print(f"    â€¢ ä¸åŒ…å«max_tokens: {'max_tokens' not in gemini_data}")
    print(f"    â€¢ ä¸åŒ…å«messages: {'messages' not in gemini_data}")
    
    # éªŒè¯æ ¼å¼å®Œå…¨ä¸åŒ
    openai_keys = set(openai_data.keys())
    gemini_keys = set(gemini_data.keys())
    
    print(f"  ğŸ“Š æ ¼å¼å·®å¼‚:")
    print(f"    â€¢ OpenAIç‹¬æœ‰å­—æ®µ: {openai_keys - gemini_keys}")
    print(f"    â€¢ Geminiç‹¬æœ‰å­—æ®µ: {gemini_keys - openai_keys}")
    
    return len(openai_keys.intersection(gemini_keys)) < len(openai_keys) / 2


def test_potential_error_scenarios():
    """æµ‹è¯•å¯èƒ½å¯¼è‡´é”™è¯¯çš„åœºæ™¯"""
    print("\nğŸ§ª æµ‹è¯•é”™è¯¯åœºæ™¯é¢„é˜²...")
    
    config = AIConfig(
        provider=AIProvider.GEMINI,
        model="gemini-1.5-pro"
    )
    
    client = AIClient(config)
    messages = [{"role": "user", "content": "Test"}]
    
    # æµ‹è¯•å„ç§å¯èƒ½å¯¼è‡´400é”™è¯¯çš„å‚æ•°
    problematic_kwargs = {
        "max_tokens": 50,
        "timeout": 30,  # åº”è¯¥è¢«æ’é™¤
        "max_retries": 3,  # åº”è¯¥è¢«æ’é™¤
        "disable_ssl_verify": True,  # åº”è¯¥è¢«æ’é™¤
        "custom_param": "test",  # åº”è¯¥è¢«åŒ…å«
        "temperature": 0.5,
        "top_p": 0.9
    }
    
    request_data = client._build_request_data(
        messages, stream=False, **problematic_kwargs
    )
    
    # éªŒè¯å±é™©å‚æ•°è¢«æ­£ç¡®å¤„ç†
    excluded_params = ["timeout", "max_retries", "disable_ssl_verify", "max_tokens"]
    for param in excluded_params:
        if param in request_data:
            print(f"    âŒ å±é™©å‚æ•° {param} ä»åœ¨è¯·æ±‚ä¸­")
            return False
        else:
            print(f"    âœ… å±é™©å‚æ•° {param} å·²æ­£ç¡®æ’é™¤")
    
    # éªŒè¯è‡ªå®šä¹‰å‚æ•°è¢«ä¿ç•™
    if "custom_param" in request_data:
        print(f"    âœ… è‡ªå®šä¹‰å‚æ•°è¢«ä¿ç•™: {request_data['custom_param']}")
    else:
        print(f"    âŒ è‡ªå®šä¹‰å‚æ•°ä¸¢å¤±")
        return False
    
    # éªŒè¯Geminiå‚æ•°åœ¨æ­£ç¡®ä½ç½®
    gen_config = request_data.get("generationConfig", {})
    if gen_config.get("maxOutputTokens") == 50:
        print(f"    âœ… max_tokensæ­£ç¡®è½¬æ¢ä¸ºmaxOutputTokens")
    else:
        print(f"    âŒ max_tokensè½¬æ¢å¤±è´¥")
        return False
    
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹GeminiåŸç”ŸAPIä¿®å¤éªŒè¯...")
    
    tests = [
        test_gemini_request_format,
        test_gemini_kwargs_handling,
        test_gemini_vs_openai_format,
        test_potential_error_scenarios
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"âŒ {test.__name__} å¤±è´¥: {e}")
            failed += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"âœ… é€šè¿‡: {passed}")
    print(f"âŒ å¤±è´¥: {failed}")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("âœ¨ GeminiåŸç”ŸAPIæ ¼å¼é—®é¢˜å·²å®Œå…¨ä¿®å¤")
        print("âœ¨ max_tokenså‚æ•°æ­£ç¡®è½¬æ¢ä¸ºmaxOutputTokens")
        print("âœ¨ æ‰€æœ‰OpenAIä¸“ç”¨å­—æ®µè¢«æ­£ç¡®æ’é™¤")
        print("âœ¨ kwargså‚æ•°å¤„ç†é€»è¾‘æ­£ç¡®")
        
        print("\nğŸ”§ ä¿®å¤è¦ç‚¹:")
        print("  â€¢ Geminiä½¿ç”¨contentsè€Œémessages")
        print("  â€¢ Geminiä½¿ç”¨generationConfigåŒ…è£…å‚æ•°")
        print("  â€¢ max_tokens â†’ generationConfig.maxOutputTokens")
        print("  â€¢ temperature â†’ generationConfig.temperature")
        print("  â€¢ top_p â†’ generationConfig.topP")
        print("  â€¢ æ’é™¤timeoutã€max_retriesç­‰å®¢æˆ·ç«¯å‚æ•°")
        
        return True
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)