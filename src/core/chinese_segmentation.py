"""
ä¸­æ–‡åˆ†è¯å’Œæ–‡æœ¬åˆ†ææ¨¡å—
åŸºäºjiebaåˆ†è¯åº“ï¼Œä¸ºCodexç³»ç»Ÿæä¾›æ™ºèƒ½æ–‡æœ¬å¤„ç†
"""

import re
import logging
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
    logger.info("jiebaä¸­æ–‡åˆ†è¯åº“åŠ è½½æˆåŠŸ - ç‰ˆæœ¬: %s", getattr(jieba, '__version__', 'unknown'))
except ImportError as e:
    JIEBA_AVAILABLE = False
    logger.warning("jiebaä¸­æ–‡åˆ†è¯åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†è¯åŠŸèƒ½")


class WordType(Enum):
    """è¯æ€§ç±»å‹"""
    NOUN = "n"          # åè¯
    VERB = "v"          # åŠ¨è¯
    ADJECTIVE = "a"     # å½¢å®¹è¯
    ADVERB = "d"        # å‰¯è¯
    PRONOUN = "r"       # ä»£è¯
    PREPOSITION = "p"   # ä»‹è¯
    CONJUNCTION = "c"   # è¿è¯
    PARTICLE = "u"      # åŠ©è¯
    NUMBER = "m"        # æ•°è¯
    PUNCTUATION = "w"   # æ ‡ç‚¹
    OTHER = "x"         # å…¶ä»–


@dataclass
class SegmentedWord:
    """åˆ†è¯ç»“æœ"""
    word: str           # è¯æ±‡
    pos: str           # è¯æ€§
    start: int         # èµ·å§‹ä½ç½®
    end: int           # ç»“æŸä½ç½®
    word_type: WordType # è¯æ±‡ç±»å‹
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        # æ ¹æ®è¯æ€§ç¡®å®šè¯æ±‡ç±»å‹
        self.word_type = self._classify_word_type(self.pos)
    
    def _classify_word_type(self, pos: str) -> WordType:
        """æ ¹æ®è¯æ€§åˆ†ç±»è¯æ±‡ç±»å‹"""
        if pos.startswith('n'):
            return WordType.NOUN
        elif pos.startswith('v'):
            return WordType.VERB
        elif pos.startswith('a'):
            return WordType.ADJECTIVE
        elif pos.startswith('d'):
            return WordType.ADVERB
        elif pos.startswith('r'):
            return WordType.PRONOUN
        elif pos.startswith('p'):
            return WordType.PREPOSITION
        elif pos.startswith('c'):
            return WordType.CONJUNCTION
        elif pos.startswith('u'):
            return WordType.PARTICLE
        elif pos.startswith('m'):
            return WordType.NUMBER
        elif pos.startswith('w'):
            return WordType.PUNCTUATION
        else:
            return WordType.OTHER


class ChineseSegmenter:
    """ä¸­æ–‡åˆ†è¯å™¨"""
    
    def __init__(self, enable_custom_dict: bool = True):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨
        
        Args:
            enable_custom_dict: æ˜¯å¦å¯ç”¨è‡ªå®šä¹‰è¯å…¸
        """
        self._custom_words = set()  # è‡ªå®šä¹‰è¯æ±‡
        self._character_names = set()  # è§’è‰²å
        self._location_names = set()   # åœ°ç‚¹å
        self._object_names = set()     # ç‰©å“å
        
        self._enable_custom_dict = enable_custom_dict
        
        if JIEBA_AVAILABLE and enable_custom_dict:
            logger.critical("ğŸ¯[JIEBA_DEBUG] æ­£åœ¨åˆå§‹åŒ–jiebaè‡ªå®šä¹‰è¯å…¸...")
            self._init_jieba()
        else:
            logger.critical("âŒ[JIEBA_DEBUG] jiebaä¸å¯ç”¨æˆ–è‡ªå®šä¹‰è¯å…¸ç¦ç”¨ - JIEBA_AVAILABLE=%s, enable_custom_dict=%s", JIEBA_AVAILABLE, enable_custom_dict)
    
    def _init_jieba(self):
        """åˆå§‹åŒ–jiebaè®¾ç½®"""
        # è®¾ç½®jiebaæ—¥å¿—çº§åˆ«
        jieba.setLogLevel(logging.WARNING)
        
        # æ·»åŠ å¸¸ç”¨çš„å°è¯´è¯æ±‡
        novel_words = [
            "æ±Ÿæ¹–", "æ­¦æ—", "é—¨æ´¾", "åŠŸå¤«", "å†…åŠŸ", "è½»åŠŸ",
            "å‰‘æ³•", "åˆ€æ³•", "æŒæ³•", "å¿ƒæ³•", "ç§˜ç±", "æ‹›å¼",
            "å¸ˆçˆ¶", "å¸ˆå…„", "å¸ˆå¼Ÿ", "å¸ˆå§", "å¸ˆå¦¹", "é•¿è€",
            "æŒé—¨", "å¼Ÿå­", "ä¾ å®¢", "å¤§ä¾ ", "å°‘ä¾ ", "å¥³ä¾ ",
            "çš‡å®«", "å®¢æ ˆ", "é…’æ¥¼", "å±±å¯¨", "æ´ç©´", "å¯†å®¤"
        ]
        
        for word in novel_words:
            jieba.add_word(word, freq=1000)
        
        logger.info(f"å·²æ·»åŠ  {len(novel_words)} ä¸ªå¸¸ç”¨å°è¯´è¯æ±‡")
    
    def add_custom_words(self, words: List[str], word_type: str = "custom"):
        """
        æ·»åŠ è‡ªå®šä¹‰è¯æ±‡
        
        Args:
            words: è¯æ±‡åˆ—è¡¨
            word_type: è¯æ±‡ç±»å‹ (character, location, object, custom)
        """
        if not words:
            return
        
        for word in words:
            if word and len(word.strip()) > 0:
                clean_word = word.strip()
                self._custom_words.add(clean_word)
                
                # æŒ‰ç±»å‹åˆ†ç±»å­˜å‚¨
                if word_type == "character":
                    self._character_names.add(clean_word)
                elif word_type == "location":
                    self._location_names.add(clean_word)
                elif word_type == "object":
                    self._object_names.add(clean_word)
                
                # å¦‚æœjiebaå¯ç”¨ï¼Œæ·»åŠ åˆ°jiebaè¯å…¸
                if JIEBA_AVAILABLE and self._enable_custom_dict:
                    jieba.add_word(clean_word, freq=2000)
        
        logger.info(f"å·²æ·»åŠ  {len(words)} ä¸ª {word_type} ç±»å‹çš„è‡ªå®šä¹‰è¯æ±‡")
    
    def segment_text(self, text: str, with_pos: bool = True) -> List[SegmentedWord]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        
        Args:
            text: å¾…åˆ†è¯çš„æ–‡æœ¬
            with_pos: æ˜¯å¦åŒ…å«è¯æ€§æ ‡æ³¨
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not text or not text.strip():
            return []
        
        results = []
        
        if JIEBA_AVAILABLE and with_pos:
            logger.critical("ğŸ¯[JIEBA_DEBUG] ä½¿ç”¨jiebaè¿›è¡Œè¯æ€§æ ‡æ³¨åˆ†è¯ï¼Œæ–‡æœ¬é•¿åº¦: %d", len(text))
            # ä½¿ç”¨jiebaè¿›è¡Œè¯æ€§æ ‡æ³¨åˆ†è¯
            words = pseg.cut(text)
            current_pos = 0
            
            for word, pos in words:
                if word.strip():  # è·³è¿‡ç©ºç™½å­—ç¬¦
                    # åœ¨åŸæ–‡ä¸­æ‰¾åˆ°è¯çš„ä½ç½®
                    start_pos = text.find(word, current_pos)
                    if start_pos != -1:
                        end_pos = start_pos + len(word)
                        
                        segmented_word = SegmentedWord(
                            word=word,
                            pos=pos,
                            start=start_pos,
                            end=end_pos,
                            word_type=WordType.OTHER  # ä¼šåœ¨__post_init__ä¸­è®¾ç½®
                        )
                        results.append(segmented_word)
                        current_pos = end_pos
        
        elif JIEBA_AVAILABLE:
            logger.critical("ğŸ¯[JIEBA_DEBUG] ä½¿ç”¨jiebaè¿›è¡Œç®€å•åˆ†è¯ï¼Œæ–‡æœ¬é•¿åº¦: %d", len(text))
            # ä½¿ç”¨jiebaè¿›è¡Œç®€å•åˆ†è¯
            words = jieba.cut(text, cut_all=False)
            current_pos = 0
            
            for word in words:
                if word.strip():
                    start_pos = text.find(word, current_pos)
                    if start_pos != -1:
                        end_pos = start_pos + len(word)
                        
                        segmented_word = SegmentedWord(
                            word=word,
                            pos="unknown",
                            start=start_pos,
                            end=end_pos,
                            word_type=WordType.OTHER
                        )
                        results.append(segmented_word)
                        current_pos = end_pos
        
        else:
            logger.critical("âŒ[JIEBA_DEBUG] jiebaä¸å¯ç”¨ï¼Œé™çº§åˆ°åŸºç¡€åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰ï¼Œæ–‡æœ¬é•¿åº¦: %d", len(text))
            # é™çº§åˆ°åŸºç¡€åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰
            results = self._basic_segment(text)
        
        return results
    
    def _basic_segment(self, text: str) -> List[SegmentedWord]:
        """åŸºç¡€åˆ†è¯ï¼ˆå½“jiebaä¸å¯ç”¨æ—¶çš„é™çº§æ–¹æ¡ˆï¼‰"""
        results = []
        
        # ç®€å•çš„åŸºäºæ­£åˆ™çš„åˆ†è¯
        # ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å•è¯ã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·
        pattern = r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+|[^\u4e00-\u9fff\w\s]'
        
        for match in re.finditer(pattern, text):
            word = match.group()
            if word.strip():
                segmented_word = SegmentedWord(
                    word=word,
                    pos="unknown",
                    start=match.start(),
                    end=match.end(),
                    word_type=WordType.OTHER
                )
                results.append(segmented_word)
        
        return results
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """
        æå–å…³é”®è¯
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªå…³é”®è¯
            
        Returns:
            (å…³é”®è¯, æƒé‡) çš„åˆ—è¡¨
        """
        if not JIEBA_AVAILABLE:
            logger.critical("âŒ[JIEBA_DEBUG] jiebaä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå…³é”®è¯æå– - JIEBA_AVAILABLE=%s", JIEBA_AVAILABLE)
            logger.warning("jiebaä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå…³é”®è¯æå–")
            return []
        
        try:
            import jieba.analyse
            # ä½¿ç”¨TF-IDFæå–å…³é”®è¯
            keywords = jieba.analyse.extract_tags(text, topK=top_k, withWeight=True)
            return keywords
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    def find_person_names(self, text: str) -> List[str]:
        """
        è¯†åˆ«äººå
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            
        Returns:
            äººååˆ—è¡¨
        """
        names = []
        
        # é¦–å…ˆæ£€æŸ¥è‡ªå®šä¹‰è§’è‰²å
        for name in self._character_names:
            if name in text:
                names.append(name)
        
        if JIEBA_AVAILABLE:
            # ä½¿ç”¨è¯æ€§æ ‡æ³¨æ‰¾äººå
            words = pseg.cut(text)
            for word, pos in words:
                # nr: äººå, nrfg: äººå_å§“, nrt: äººå_å­—
                if pos in ['nr', 'nrfg', 'nrt'] and len(word) >= 2:
                    if word not in names:
                        names.append(word)
        
        return names
    
    def find_place_names(self, text: str) -> List[str]:
        """
        è¯†åˆ«åœ°å
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            
        Returns:
            åœ°ååˆ—è¡¨
        """
        places = []
        
        # é¦–å…ˆæ£€æŸ¥è‡ªå®šä¹‰åœ°ç‚¹å
        for place in self._location_names:
            if place in text:
                places.append(place)
        
        if JIEBA_AVAILABLE:
            # ä½¿ç”¨è¯æ€§æ ‡æ³¨æ‰¾åœ°å
            words = pseg.cut(text)
            for word, pos in words:
                # ns: åœ°å, nt: æœºæ„å›¢ä½“å
                if pos in ['ns', 'nt'] and len(word) >= 2:
                    if word not in places:
                        places.append(word)
        
        return places
    
    def analyze_text_structure(self, text: str) -> Dict[str, any]:
        """
        åˆ†ææ–‡æœ¬ç»“æ„
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            
        Returns:
            æ–‡æœ¬ç»“æ„åˆ†æç»“æœ
        """
        segments = self.segment_text(text, with_pos=True)
        
        # ç»Ÿè®¡å„ç§è¯æ€§
        pos_count = {}
        word_types = {}
        
        for seg in segments:
            pos_count[seg.pos] = pos_count.get(seg.pos, 0) + 1
            word_types[seg.word_type] = word_types.get(seg.word_type, 0) + 1
        
        # æå–å®ä½“
        persons = self.find_person_names(text)
        places = self.find_place_names(text)
        keywords = self.extract_keywords(text, top_k=10)
        
        return {
            'total_words': len(segments),
            'total_characters': len(text),
            'pos_distribution': pos_count,
            'word_type_distribution': {wt.name: count for wt, count in word_types.items()},
            'persons': persons,
            'places': places,
            'keywords': [kw[0] for kw in keywords],  # åªè¿”å›å…³é”®è¯ï¼Œä¸åŒ…å«æƒé‡
            'segments': segments[:20]  # è¿”å›å‰20ä¸ªåˆ†è¯ç»“æœä½œä¸ºç¤ºä¾‹
        }
    
    def update_custom_dictionary(self, codex_entries: List['CodexEntry']):
        """
        æ ¹æ®Codexæ¡ç›®æ›´æ–°è‡ªå®šä¹‰è¯å…¸
        
        Args:
            codex_entries: Codexæ¡ç›®åˆ—è¡¨
        """
        if not codex_entries:
            return
        
        characters = []
        locations = []
        objects = []
        
        for entry in codex_entries:
            words = [entry.title] + (entry.aliases or [])
            
            if entry.entry_type.name == 'CHARACTER':
                characters.extend(words)
            elif entry.entry_type.name == 'LOCATION':
                locations.extend(words)
            elif entry.entry_type.name == 'OBJECT':
                objects.extend(words)
        
        # æ‰¹é‡æ·»åŠ è¯æ±‡
        if characters:
            self.add_custom_words(characters, "character")
        if locations:
            self.add_custom_words(locations, "location")
        if objects:
            self.add_custom_words(objects, "object")
        
        logger.info(f"å·²æ›´æ–°è‡ªå®šä¹‰è¯å…¸: {len(characters)} ä¸ªè§’è‰², {len(locations)} ä¸ªåœ°ç‚¹, {len(objects)} ä¸ªç‰©å“")


# å…¨å±€åˆ†è¯å™¨å®ä¾‹
_global_segmenter: Optional[ChineseSegmenter] = None


def get_segmenter() -> ChineseSegmenter:
    """è·å–å…¨å±€åˆ†è¯å™¨å®ä¾‹"""
    global _global_segmenter
    if _global_segmenter is None:
        _global_segmenter = ChineseSegmenter()
    return _global_segmenter


def segment_text(text: str, with_pos: bool = True) -> List[SegmentedWord]:
    """ä¾¿åˆ©å‡½æ•°ï¼šå¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯"""
    return get_segmenter().segment_text(text, with_pos)


def extract_keywords(text: str, top_k: int = 10) -> List[Tuple[str, float]]:
    """ä¾¿åˆ©å‡½æ•°ï¼šæå–å…³é”®è¯"""
    return get_segmenter().extract_keywords(text, top_k)


def analyze_text(text: str) -> Dict[str, any]:
    """ä¾¿åˆ©å‡½æ•°ï¼šåˆ†ææ–‡æœ¬ç»“æ„"""
    return get_segmenter().analyze_text_structure(text)