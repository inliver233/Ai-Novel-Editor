"""
SQLiteå‘é‡å­˜å‚¨ç®¡ç†å™¨
"""
import sqlite3
import json
import logging
import hashlib
# import pickle  # ç§»é™¤pickleï¼Œä½¿ç”¨JSONåºåˆ—åŒ–
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# å°è¯•å¯¼å…¥numpy
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    # å¦‚æœæ²¡æœ‰numpyï¼Œæˆ‘ä»¬ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
    class FakeNp:
        @staticmethod
        def array(data):
            return data
        
        @staticmethod
        def dot(a, b):
            return sum(x * y for x, y in zip(a, b))
        
        @staticmethod
        def linalg_norm(vec):
            return sum(x * x for x in vec) ** 0.5
    
    np = FakeNp()

logger = logging.getLogger(__name__)


class SQLiteVectorStore:
    """SQLiteå‘é‡å­˜å‚¨å®ç°"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self._init_database()
        
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ–‡æ¡£åµŒå…¥è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS document_embeddings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    document_id TEXT NOT NULL,
                    chunk_index INTEGER NOT NULL,
                    chunk_text TEXT NOT NULL,
                    start_pos INTEGER NOT NULL,
                    end_pos INTEGER NOT NULL,
                    embedding BLOB NOT NULL,
                    embedding_model TEXT,
                    metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(document_id, chunk_index)
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_document_embeddings_doc_id 
                ON document_embeddings(document_id)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_document_embeddings_created 
                ON document_embeddings(created_at)
            """)
            
            # RAGé…ç½®è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS rag_config (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    project_id TEXT NOT NULL,
                    config_data TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(project_id)
                )
            """)
            
            # æœç´¢å†å²è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†æå’Œä¼˜åŒ–ï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS search_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query TEXT NOT NULL,
                    results_count INTEGER,
                    search_time_ms INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
    
    def store_embedding(self, document_id: str, chunk_index: int, 
                       chunk_text: str, start_pos: int, end_pos: int,
                       embedding: List[float], embedding_model: str = None,
                       metadata: Dict[str, Any] = None) -> int:
        """å­˜å‚¨åµŒå…¥å‘é‡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # åºåˆ—åŒ–åµŒå…¥å‘é‡å’Œå…ƒæ•°æ®
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨è¿›è¡ŒJSONåºåˆ—åŒ–
            if NUMPY_AVAILABLE and isinstance(embedding, np.ndarray):
                embedding_list = embedding.tolist()
            else:
                embedding_list = list(embedding)
            embedding_json = json.dumps(embedding_list)
            metadata_json = json.dumps(metadata) if metadata else None
            
            # æ’å…¥æˆ–æ›´æ–°
            cursor.execute("""
                INSERT OR REPLACE INTO document_embeddings 
                (document_id, chunk_index, chunk_text, start_pos, end_pos, 
                 embedding, embedding_model, metadata, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (document_id, chunk_index, chunk_text, start_pos, end_pos,
                  embedding_json, embedding_model, metadata_json))
            
            conn.commit()
            return cursor.lastrowid
    
    def store_embeddings_batch(self, embeddings: List[Dict[str, Any]]) -> List[int]:
        """æ‰¹é‡å­˜å‚¨åµŒå…¥å‘é‡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            ids = []
            for emb_data in embeddings:
                # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨è¿›è¡ŒJSONåºåˆ—åŒ–
                embedding = emb_data['embedding']
                if NUMPY_AVAILABLE and isinstance(embedding, np.ndarray):
                    embedding_list = embedding.tolist()
                else:
                    embedding_list = list(embedding)
                embedding_json = json.dumps(embedding_list)
                metadata_json = json.dumps(emb_data.get('metadata')) if emb_data.get('metadata') else None
                
                cursor.execute("""
                    INSERT OR REPLACE INTO document_embeddings 
                    (document_id, chunk_index, chunk_text, start_pos, end_pos, 
                     embedding, embedding_model, metadata, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                """, (emb_data['document_id'], emb_data['chunk_index'], 
                      emb_data['chunk_text'], emb_data['start_pos'], 
                      emb_data['end_pos'], embedding_json,
                      emb_data.get('embedding_model'), metadata_json))
                
                ids.append(cursor.lastrowid)
            
            conn.commit()
            return ids
    
    def get_embeddings_by_document(self, document_id: str) -> List[Dict[str, Any]]:
        """è·å–æ–‡æ¡£çš„æ‰€æœ‰åµŒå…¥å‘é‡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT id, chunk_index, chunk_text, start_pos, end_pos,
                       embedding, embedding_model, metadata, created_at, updated_at
                FROM document_embeddings
                WHERE document_id = ?
                ORDER BY chunk_index
            """, (document_id,))
            
            results = []
            for row in cursor.fetchall():
                # ä»JSONååºåˆ—åŒ–åµŒå…¥å‘é‡
                embedding_list = json.loads(row[5])
                if NUMPY_AVAILABLE:
                    embedding = np.array(embedding_list)
                else:
                    embedding = embedding_list
                metadata = json.loads(row[7]) if row[7] else None
                
                results.append({
                    'id': row[0],
                    'document_id': document_id,
                    'chunk_index': row[1],
                    'chunk_text': row[2],
                    'start_pos': row[3],
                    'end_pos': row[4],
                    'embedding': embedding.tolist(),
                    'embedding_model': row[6],
                    'metadata': metadata,
                    'created_at': row[8],
                    'updated_at': row[9]
                })
            
            return results
    
    def get_all_embeddings(self, limit: int = None) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰åµŒå…¥å‘é‡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            query = """
                SELECT id, document_id, chunk_index, chunk_text, start_pos, end_pos,
                       embedding, embedding_model, metadata, created_at, updated_at
                FROM document_embeddings
                ORDER BY document_id, chunk_index
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cursor.execute(query)
            
            results = []
            for row in cursor.fetchall():
                # ä»JSONååºåˆ—åŒ–åµŒå…¥å‘é‡
                embedding_list = json.loads(row[6])
                if NUMPY_AVAILABLE:
                    embedding = np.array(embedding_list)
                else:
                    embedding = embedding_list
                metadata = json.loads(row[8]) if row[8] else None
                
                results.append({
                    'id': row[0],
                    'document_id': row[1],
                    'chunk_index': row[2],
                    'chunk_text': row[3],
                    'start_pos': row[4],
                    'end_pos': row[5],
                    'embedding': embedding.tolist(),
                    'embedding_model': row[7],
                    'metadata': metadata,
                    'created_at': row[9],
                    'updated_at': row[10]
                })
            
            return results
    
    def delete_document_embeddings(self, document_id: str) -> int:
        """åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰åµŒå…¥å‘é‡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                DELETE FROM document_embeddings
                WHERE document_id = ?
            """, (document_id,))
            
            conn.commit()
            return cursor.rowcount

    def delete_document(self, document_id: str) -> int:
        """åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰åµŒå…¥å‘é‡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.delete_document_embeddings(document_id)
    
    def document_exists(self, document_id: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ç»è¢«ç´¢å¼•"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    SELECT COUNT(*) FROM document_embeddings
                    WHERE document_id = ?
                """, (document_id,))
                
                count = cursor.fetchone()[0]
                return count > 0
                
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡æ¡£ç´¢å¼•çŠ¶æ€å¤±è´¥ {document_id}: {e}")
            return False
    
    def similarity_search_ultra_fast(self, query_text: str, limit: int = 1) -> str:
        """è¶…å¿«é€Ÿç›¸ä¼¼åº¦æœç´¢ï¼ˆé˜²å¡æ­»ä¸“ç”¨ï¼‰- 800msä¸¥æ ¼è¶…æ—¶"""
        import time
        start_time = time.time()
        
        try:
            # ç«‹å³æ£€æŸ¥æ•°æ®åº“è¿æ¥
            with sqlite3.connect(self.db_path, timeout=0.5) as conn:  # 500msè¿æ¥è¶…æ—¶
                cursor = conn.cursor()
                
                # æ”¹è¿›çš„æ–‡æœ¬æœç´¢é€»è¾‘ - åˆ†ç¦»å…³é”®è¯è¿›è¡Œæ›´ç²¾ç¡®çš„åŒ¹é…
                keywords = []
                
                # ä¿®å¤å…³é”®è¯æå–é€»è¾‘
                import re
                
                # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦
                cleaned_query = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€,.\s]+', '', query_text)
                
                logger.info(f"[SEARCH] åŸå§‹æŸ¥è¯¢: '{query_text}', æ¸…ç†å: '{cleaned_query}'")
                
                if len(cleaned_query) > 6:
                    # å¯¹äºè¾ƒé•¿çš„æŸ¥è¯¢ï¼Œå°è¯•æŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
                    # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰è¯æ±‡ï¼ˆä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ï¼‰
                    stop_words = ['çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'äº†', 'ç€', 'è¿‡', 'ç­‰', 'ä¸»é¢˜', 'å†…å®¹', 'å…³äº', 'ä»', 'è¢«', 'åˆ°', 'ä»–', 'å¥¹', 'æˆ‘']
                    
                    # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼šå°è¯•æå–äººåã€åœ°åç­‰å…³é”®ä¿¡æ¯
                    # æŸ¥æ‰¾å¯èƒ½çš„äººåï¼ˆ2-3ä¸ªè¿ç»­æ±‰å­—ï¼‰
                    name_pattern = re.findall(r'[\u4e00-\u9fff]{2,3}', cleaned_query)
                    
                    # è¿‡æ»¤åœç”¨è¯
                    filtered_words = [word for word in name_pattern if word not in stop_words and len(word) >= 2]
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬çš„ç‰‡æ®µ
                    if filtered_words:
                        keywords = filtered_words[:3]  # æœ€å¤šå–3ä¸ªå…³é”®è¯
                    else:
                        # ä¿®å¤ä»£ç  - æ™ºèƒ½åˆ†å‰²æ›¿æ¢æœºæ¢°åˆ†å‰²
                        if len(cleaned_query) >= 4:
                            # ä½¿ç”¨åŸºäºè¯é¢‘å’Œè¯­ä¹‰çš„åˆ†å‰²
                            try:
                                # å°è¯•ä½¿ç”¨jiebaåˆ†è¯
                                import jieba
                                logger.critical("ğŸ¯[JIEBA_DEBUG] sqlite_vector_storeä¸­jiebaå¯¼å…¥æˆåŠŸï¼Œå‡†å¤‡åˆ†è¯å¤„ç†")
                                words = list(jieba.cut(cleaned_query))
                                logger.critical("ğŸ¯[JIEBA_DEBUG] jiebaåˆ†è¯ç»“æœ: %s", words)
                                # æ‰©å±•åœç”¨è¯åˆ—è¡¨
                                stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'äº†', 'ç€', 'è¿‡', 'ç­‰', 'ä¸»é¢˜', 'å†…å®¹', 'å…³äº', 'ä»', 'è¢«', 'åˆ°',
                                            'ä»–', 'å¥¹', 'æˆ‘', 'ä½ ', 'å®ƒ', 'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä¸€ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ',
                                            'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'æ—¶å€™', 'åœ°æ–¹', 'ä¸œè¥¿', 'äº‹æƒ…', 'é—®é¢˜', 'æ–¹é¢', 'æƒ…å†µ'}
                                filtered_words = [w for w in words if len(w) >= 2 and w not in stop_words]
                                if filtered_words:
                                    keywords = filtered_words[:3]
                                else:
                                    # é™çº§åˆ°æ”¹è¿›çš„æ­£åˆ™æå–
                                    chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', cleaned_query)
                                    keywords = [w for w in chinese_words if w not in stop_words][:3]
                            except Exception as e:
                                logger.critical("âŒ[JIEBA_DEBUG] sqlite_vector_storeä¸­jiebaåˆ†è¯å¤±è´¥: %s", e)
                                # æœ€åé™çº§åˆ°æ”¹è¿›çš„å­—ç¬¦ç»„åˆ
                                chars = re.findall(r'[\u4e00-\u9fff]', cleaned_query)
                                keywords = []
                                for i in range(len(chars)-1):
                                    word = chars[i] + chars[i+1]
                                    if word not in stop_words:
                                        keywords.append(word)
                                        if len(keywords) >= 3:
                                            break
                elif len(cleaned_query) >= 2:
                    keywords = [cleaned_query]
                
                # å¦‚æœå…³é”®è¯æå–å¤±è´¥ï¼Œå°è¯•AIå…³é”®è¯æå–
                if not keywords and len(query_text.strip()) >= 2:
                    logger.info(f"[SEARCH] ä¼ ç»Ÿåˆ†è¯å¤±è´¥ï¼Œå°è¯•AIå…³é”®è¯æå–...")
                    ai_keywords = self._extract_keywords_with_ai(query_text)
                    if ai_keywords:
                        keywords = ai_keywords
                        logger.info(f"[SEARCH] AIå…³é”®è¯æå–æˆåŠŸ: {keywords}")
                    else:
                        # æœ€åå›é€€ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢çš„ç‰‡æ®µ
                        original_clean = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€,.\s]+', '', query_text)
                        if len(original_clean) >= 2:
                            keywords = [original_clean[:4]]  # å–å‰4ä¸ªå­—ç¬¦
                            logger.info(f"[SEARCH] ä½¿ç”¨åŸå§‹æŸ¥è¯¢ç‰‡æ®µ: {keywords}")
                
                logger.info(f"[SEARCH] æå–çš„å…³é”®è¯: {keywords}")
                
                # æ„å»ºæ›´çµæ´»çš„æœç´¢æ¡ä»¶
                search_conditions = []
                search_params = []
                
                for keyword in keywords:
                    if len(keyword) >= 2:
                        search_conditions.append("chunk_text LIKE ?")
                        search_params.append(f'%{keyword}%')
                
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                if not search_conditions:
                    search_conditions = ["chunk_text LIKE ?"]
                    search_params = [f'%{query_text}%']
                
                # ç§»é™¤é‡å¤çš„æ¡ä»¶æ£€æŸ¥ï¼Œé¿å…keywordå˜é‡é”™è¯¯
                
                # æ·»åŠ limitå‚æ•°
                search_params.append(limit * 3)  # è·å–æ›´å¤šå€™é€‰ï¼Œç„¶åç­›é€‰
                
                where_clause = " OR ".join(search_conditions)
                
                cursor.execute(f"""
                    SELECT chunk_text, document_id, chunk_index,
                           LENGTH(chunk_text) as text_length
                    FROM document_embeddings 
                    WHERE {where_clause}
                    ORDER BY text_length ASC, chunk_index ASC
                    LIMIT ?
                """, search_params)
                
                # 800msè¶…æ—¶æ£€æŸ¥
                if time.time() - start_time > 0.8:
                    logger.warning("è¶…å¿«é€Ÿæœç´¢è¶…æ—¶ï¼ˆ800msï¼‰")
                    return ""
                
                results = cursor.fetchall()
                
                if results:
                    logger.info(f"[SEARCH] æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ")
                    
                    # é€‰æ‹©æœ€ä½³åŒ¹é…ç»“æœ
                    best_result = None
                    best_score = 0
                    
                    for chunk_text, doc_id, chunk_index, text_length in results:
                        # è®¡ç®—åŒ¹é…åº¦åˆ†æ•°
                        score = 0
                        text_lower = chunk_text.lower()
                        
                        # å…³é”®è¯åŒ¹é…åˆ†æ•°
                        for keyword in keywords:
                            if keyword.lower() in text_lower:
                                score += 1
                        
                        # é•¿åº¦é€‚ä¸­çš„æ–‡æœ¬ä¼˜å…ˆ
                        if 50 <= text_length <= 300:
                            score += 0.5
                        
                        # æ—©æœŸç« èŠ‚ä¼˜å…ˆ
                        if chunk_index <= 2:
                            score += 0.3
                        
                        if score > best_score:
                            best_score = score
                            best_result = chunk_text
                    
                    if best_result:
                        # é™åˆ¶è¿”å›é•¿åº¦
                        result = best_result[:200] if len(best_result) > 200 else best_result
                        logger.info(f"[SEARCH] æœç´¢æˆåŠŸ: æœ€ä½³åŒ¹é…åˆ†æ•°={best_score:.1f}, ç»“æœé•¿åº¦={len(result)}, ç”¨æ—¶={time.time() - start_time:.3f}s")
                        return result
                    else:
                        # å¦‚æœæ²¡æœ‰è®¡ç®—å‡ºæœ€ä½³ç»“æœï¼Œè¿”å›ç¬¬ä¸€ä¸ª
                        chunk_text = results[0][0]
                        result = chunk_text[:200] if len(chunk_text) > 200 else chunk_text
                        logger.info(f"[SEARCH] ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ: é•¿åº¦={len(result)}, ç”¨æ—¶={time.time() - start_time:.3f}s")
                        return result
                
                logger.info(f"[SEARCH] æ— åŒ¹é…ç»“æœ, ç”¨æ—¶={time.time() - start_time:.3f}s")
                return ""
                
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"[SEARCH] æœç´¢å¤±è´¥ï¼ˆç”¨æ—¶ {elapsed:.3f}ç§’ï¼‰: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return ""
    
    def similarity_search_fast(self, query_text: str, limit: int = 1) -> str:
        """å¿«é€Ÿç›¸ä¼¼åº¦æœç´¢ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.similarity_search_ultra_fast(query_text, limit)
    
    def similarity_search(self, query_embedding: List[float], 
                         limit: int = 10,
                         min_similarity: float = 0.0) -> List[Tuple[Dict[str, Any], float]]:
        """æ”¹è¿›çš„ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦ä¸¥æ ¼è¶…æ—¶å’Œæ€§èƒ½ä¼˜åŒ–ï¼‰
        
        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„å®ç°ï¼ŒåŒ…å«ä¸¥æ ¼çš„è¶…æ—¶æ§åˆ¶å’Œå¿«é€Ÿå¤±è´¥æœºåˆ¶ã€‚
        å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œå»ºè®®ä½¿ç”¨sqlite-vssç­‰ä¸“é—¨çš„å‘é‡æœç´¢æ‰©å±•ã€‚
        """
        import time
        start_time = time.time()
        
        # è®¾ç½®ä¸¥æ ¼çš„è¶…æ—¶é™åˆ¶
        MAX_SEARCH_TIME = 0.8  # 800msä¸¥æ ¼è¶…æ—¶
        
        def timeout_check(stage: str = ""):
            elapsed = time.time() - start_time
            if elapsed > MAX_SEARCH_TIME:
                logger.warning(f"å‘é‡æœç´¢åœ¨{stage}é˜¶æ®µè¶…æ—¶: {elapsed:.2f}ç§’")
                raise TimeoutError(f"æœç´¢è¶…æ—¶: {elapsed:.2f}ç§’")
            return elapsed
        
        query_vec = np.array(query_embedding)
        
        # æä¸¥æ ¼çš„æ€§èƒ½ä¿æŠ¤
        limit = min(limit, 5)  # æœ€å¤š5ä¸ªç»“æœ
        
        try:
            # å¿«é€Ÿæ•°æ®åº“è¿æ¥æ£€æŸ¥
            timeout_check("è¿æ¥æ£€æŸ¥")
            
            # ä½¿ç”¨åˆ†é¡µè·å–ä»¥é¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ•°æ®
            page_size = min(100, limit * 5)  # å‡å°åˆ†é¡µå¤§å°
            
            with sqlite3.connect(self.db_path, timeout=0.5) as conn:  # 500msæ•°æ®åº“è¶…æ—¶
                cursor = conn.cursor()
                
                # ä¼˜åŒ–æŸ¥è¯¢ï¼šåªè·å–å¿…è¦å­—æ®µï¼Œé™åˆ¶è¿”å›è¡Œæ•°
                cursor.execute("""
                    SELECT id, document_id, chunk_index, chunk_text, start_pos, end_pos,
                           embedding, metadata
                    FROM document_embeddings
                    ORDER BY updated_at DESC
                    LIMIT ?
                """, (page_size,))
                
                timeout_check("æ•°æ®åº“æŸ¥è¯¢")
                
                rows = cursor.fetchall()
                
        except sqlite3.OperationalError as e:
            logger.error(f"SQLiteæ“ä½œå¤±è´¥: {e}")
            return []
        except TimeoutError:
            logger.warning("æ•°æ®åº“æŸ¥è¯¢è¶…æ—¶ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        except Exception as e:
            logger.error(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
            return []
        
        if not rows:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå¸¦ä¸¥æ ¼æ€§èƒ½æ§åˆ¶ï¼‰
        results = []
        processed_count = 0
        max_process = min(len(rows), 50)  # æœ€å¤šå¤„ç†50ä¸ªå‘é‡
        
        for row in rows[:max_process]:
            processed_count += 1
            
            # é¢‘ç¹çš„è¶…æ—¶æ£€æŸ¥
            if processed_count % 10 == 0:  # æ¯10ä¸ªæ£€æŸ¥ä¸€æ¬¡
                try:
                    timeout_check(f"ç›¸ä¼¼åº¦è®¡ç®—({processed_count})")
                except TimeoutError:
                    break
            
            try:
                # å¿«é€Ÿè§£ææ•°æ®
                id_val, document_id, chunk_index, chunk_text, start_pos, end_pos, embedding_blob, metadata_json = row
                
                # å¿«é€Ÿååºåˆ—åŒ–åµŒå…¥å‘é‡
                doc_vec_list = json.loads(embedding_blob)
                if NUMPY_AVAILABLE:
                    doc_vec = np.array(doc_vec_list)
                else:
                    doc_vec = doc_vec_list
                
                # å¿«é€Ÿè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                dot_product = np.dot(query_vec, doc_vec)
                norm_query = np.linalg.norm(query_vec)
                norm_doc = np.linalg.norm(doc_vec)
                
                if norm_query == 0 or norm_doc == 0:
                    continue
                
                cosine_similarity = float(dot_product / (norm_query * norm_doc))
                
                # åŸºç¡€ç›¸ä¼¼åº¦è¿‡æ»¤
                if cosine_similarity < min_similarity:
                    continue
                
                # æ„å»ºç»“æœæ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                emb_data = {
                    'id': id_val,
                    'document_id': document_id,
                    'chunk_index': chunk_index,
                    'chunk_text': chunk_text[:500],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                    'start_pos': start_pos,
                    'end_pos': end_pos,
                    'metadata': json.loads(metadata_json) if metadata_json else None
                }
                
                # ä½¿ç”¨ç®€åŒ–çš„å¢å¼ºåˆ†æ•°è®¡ç®—
                enhanced_score = cosine_similarity * 0.9 + (0.1 if chunk_index == 0 else 0.0)
                
                results.append((emb_data, enhanced_score))
                
                # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœï¼Œæå‰é€€å‡º
                if len(results) >= limit:
                    break
                
            except Exception as e:
                # è·³è¿‡é”™è¯¯çš„å‘é‡ï¼Œç»§ç»­å¤„ç†
                logger.debug(f"å¤„ç†å‘é‡æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                continue
        
        # å¿«é€Ÿæ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        # è®°å½•æ€§èƒ½ä¿¡æ¯
        total_time = time.time() - start_time
        if total_time > 0.5:
            logger.warning(f"å‘é‡æœç´¢è€—æ—¶: {total_time:.2f}ç§’ï¼Œå¤„ç†äº† {processed_count} ä¸ªå‘é‡")
        else:
            logger.debug(f"å‘é‡æœç´¢å®Œæˆ: {total_time:.3f}ç§’ï¼Œ{len(results)} ä¸ªç»“æœ")
        
        # è¿”å›å‰Nä¸ªç»“æœ
        return results[:limit]
    
    def _calculate_enhanced_similarity_fast(self, emb_data: Dict[str, Any], 
                                          base_similarity: float) -> float:
        """è®¡ç®—å¢å¼ºçš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰"""
        # åŸºç¡€ä½™å¼¦ç›¸ä¼¼åº¦æƒé‡
        enhanced_score = base_similarity * 0.8
        
        # ç®€åŒ–çš„é•¿åº¦è´¨é‡åˆ†æ•°
        chunk_text = emb_data.get('chunk_text', '')
        text_length = len(chunk_text)
        
        # ç†æƒ³é•¿åº¦èŒƒå›´ï¼š100-800å­—ç¬¦
        if 100 <= text_length <= 800:
            length_bonus = 0.1
        elif 50 <= text_length <= 1200:
            length_bonus = 0.05
        else:
            length_bonus = 0.0
        
        # ç®€åŒ–çš„ä½ç½®åŠ åˆ†
        chunk_index = emb_data.get('chunk_index', 0)
        position_bonus = 0.05 if chunk_index == 0 else 0.02 if chunk_index == 1 else 0.0
        
        # ç»„åˆæœ€ç»ˆåˆ†æ•°
        final_score = enhanced_score + length_bonus + position_bonus
        
        return min(final_score, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1.0
    
    def _calculate_enhanced_similarity(self, emb_data: Dict[str, Any], 
                                     base_similarity: float,
                                     query_embedding: List[float]) -> float:
        """è®¡ç®—å¢å¼ºçš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        # åŸºç¡€ä½™å¼¦ç›¸ä¼¼åº¦æƒé‡
        enhanced_score = base_similarity * 0.7
        
        # 1. æ–‡æœ¬é•¿åº¦è´¨é‡åˆ†æ•°ï¼ˆé€‚ä¸­é•¿åº¦çš„æ–‡æœ¬å—é€šå¸¸è´¨é‡æ›´é«˜ï¼‰
        chunk_text = emb_data.get('chunk_text', '')
        text_length = len(chunk_text)
        
        # ç†æƒ³é•¿åº¦èŒƒå›´ï¼š100-800å­—ç¬¦
        if 100 <= text_length <= 800:
            length_bonus = 0.1
        elif 50 <= text_length < 100 or 800 < text_length <= 1200:
            length_bonus = 0.05
        else:
            length_bonus = 0.0
        
        # 2. æ–‡æœ¬å†…å®¹è´¨é‡åˆ†æ•°ï¼ˆé¿å…ç©ºç™½ã€é‡å¤å†…å®¹ï¼‰
        content_quality = self._assess_content_quality(chunk_text)
        quality_bonus = content_quality * 0.1
        
        # 3. ä½ç½®ç›¸å…³æ€§åˆ†æ•°ï¼ˆæ–‡æ¡£å¼€å¤´å’Œç»“å°¾çš„é‡è¦æ€§ï¼‰
        position_bonus = self._calculate_position_bonus(emb_data)
        
        # 4. å¤šæ ·æ€§æƒ©ç½šï¼ˆé¿å…è¿”å›è¿‡äºç›¸ä¼¼çš„å—ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ç»´æŠ¤ä¸€ä¸ªå·²é€‰æ‹©å—çš„åˆ—è¡¨
        
        # ç»„åˆæœ€ç»ˆåˆ†æ•°
        final_score = enhanced_score + length_bonus + quality_bonus + position_bonus
        
        return min(final_score, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1.0
    
    def _assess_content_quality(self, text: str) -> float:
        """è¯„ä¼°æ–‡æœ¬å†…å®¹è´¨é‡"""
        if not text or len(text.strip()) < 10:
            return 0.0
        
        # è®¡ç®—æœ‰æ•ˆå­—ç¬¦æ¯”ä¾‹
        import re
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        clean_text = re.sub(r'\s+', ' ', text.strip())
        
        # æ£€æŸ¥é‡å¤å­—ç¬¦æ¨¡å¼
        repeat_pattern = re.compile(r'(.)\1{5,}')  # è¿ç»­6ä¸ªä»¥ä¸Šç›¸åŒå­—ç¬¦
        if repeat_pattern.search(clean_text):
            return 0.3  # é™ä½è´¨é‡åˆ†æ•°
        
        # æ£€æŸ¥å­—ç¬¦å¤šæ ·æ€§
        unique_chars = len(set(clean_text.lower()))
        total_chars = len(clean_text)
        diversity_ratio = unique_chars / total_chars if total_chars > 0 else 0
        
        # è´¨é‡åˆ†æ•°ï¼š0.3åŸºç¡€åˆ† + 0.7*å¤šæ ·æ€§æ¯”ä¾‹
        quality_score = 0.3 + 0.7 * min(diversity_ratio * 3, 1.0)
        
        return quality_score
    
    def _calculate_position_bonus(self, emb_data: Dict[str, Any]) -> float:
        """è®¡ç®—ä½ç½®ç›¸å…³æ€§åŠ åˆ†"""
        chunk_index = emb_data.get('chunk_index', 0)
        
        # æ–‡æ¡£å¼€å¤´çš„å—ç»™äºˆå°å¹…åŠ åˆ†
        if chunk_index == 0:
            return 0.05
        elif chunk_index == 1:
            return 0.02
        else:
            return 0.0
    
    def save_rag_config(self, project_id: str, config: Dict[str, Any]):
        """ä¿å­˜RAGé…ç½®"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            config_json = json.dumps(config)
            
            cursor.execute("""
                INSERT OR REPLACE INTO rag_config 
                (project_id, config_data, updated_at)
                VALUES (?, ?, CURRENT_TIMESTAMP)
            """, (project_id, config_json))
            
            conn.commit()
    
    def get_rag_config(self, project_id: str) -> Optional[Dict[str, Any]]:
        """è·å–RAGé…ç½®"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT config_data FROM rag_config
                WHERE project_id = ?
            """, (project_id,))
            
            row = cursor.fetchone()
            if row:
                return json.loads(row[0])
            return None
    
    def log_search(self, query: str, results_count: int, search_time_ms: int):
        """è®°å½•æœç´¢å†å²"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO search_history (query, results_count, search_time_ms)
                VALUES (?, ?, ?)
            """, (query, results_count, search_time_ms))
            
            conn.commit()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ–‡æ¡£æ•°é‡
            cursor.execute("""
                SELECT COUNT(DISTINCT document_id) FROM document_embeddings
            """)
            doc_count = cursor.fetchone()[0]
            
            # åµŒå…¥å‘é‡æ€»æ•°
            cursor.execute("""
                SELECT COUNT(*) FROM document_embeddings
            """)
            embedding_count = cursor.fetchone()[0]
            
            # å·²ç´¢å¼•çš„æ–‡æ¡£åˆ—è¡¨
            cursor.execute("""
                SELECT document_id FROM document_embeddings
                GROUP BY document_id
                ORDER BY MAX(updated_at) DESC
            """)
            indexed_docs = [row[0] for row in cursor.fetchall()]
            
            # è°ƒè¯•ï¼šæ‰“å°æŸ¥è¯¢ç»“æœ
            logger.info(f"SQLiteç»Ÿè®¡æŸ¥è¯¢ç»“æœ: æ–‡æ¡£æ•°={doc_count}, åµŒå…¥æ•°={embedding_count}, æ–‡æ¡£åˆ—è¡¨={indexed_docs}")
            
            # æœ€åæ›´æ–°æ—¶é—´
            cursor.execute("""
                SELECT MAX(updated_at) FROM document_embeddings
            """)
            last_updated = cursor.fetchone()[0] or ''
            
            # æœç´¢æ¬¡æ•°
            cursor.execute("""
                SELECT COUNT(*) FROM search_history
            """)
            search_count = cursor.fetchone()[0]
            
            # å¹³å‡æœç´¢æ—¶é—´
            cursor.execute("""
                SELECT AVG(search_time_ms) FROM search_history
            """)
            avg_search_time = cursor.fetchone()[0] or 0
            
            # ä¼°ç®—ç´¢å¼•å¤§å°ï¼ˆMBï¼‰
            import os
            try:
                file_size = os.path.getsize(self.db_path)
                size_mb = file_size / (1024 * 1024)
            except:
                size_mb = 0.0
            
            return {
                'total_documents': doc_count,
                'total_chunks': embedding_count,
                'indexed_documents': indexed_docs,
                'last_updated': last_updated,
                'index_size_mb': round(size_mb, 2),
                'search_count': search_count,
                'avg_search_time_ms': round(avg_search_time, 2)
            }
    
    def store_embeddings(self, document_id: str, chunks, embeddings: List[List[float]], content: str = None):
        """å­˜å‚¨æ–‡æ¡£çš„æ‰€æœ‰åµŒå…¥å‘é‡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        if not chunks or not embeddings:
            logger.warning(f"No chunks or embeddings to store for document {document_id}")
            return
        
        if len(chunks) != len(embeddings):
            logger.error(f"Chunks and embeddings count mismatch: {len(chunks)} vs {len(embeddings)}")
            return
        
        # è®¡ç®—å†…å®¹å“ˆå¸Œ
        content_hash = None
        if content:
            content_hash = hashlib.md5(content.encode()).hexdigest()
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            for chunk, embedding in zip(chunks, embeddings):
                # åºåˆ—åŒ–åµŒå…¥å‘é‡ï¼ˆä½¿ç”¨JSONä»£æ›¿pickleï¼‰
                if NUMPY_AVAILABLE:
                    embedding_list = np.array(embedding).tolist()
                else:
                    embedding_list = list(embedding)
                embedding_blob = json.dumps(embedding_list)
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {}
                if hasattr(chunk, 'metadata') and chunk.metadata:
                    metadata.update(chunk.metadata)
                
                # æ·»åŠ å†…å®¹å“ˆå¸Œåˆ°å…ƒæ•°æ®
                if content_hash:
                    metadata['content_hash'] = content_hash
                
                metadata_json = json.dumps(metadata) if metadata else None
                
                # æ’å…¥æˆ–æ›´æ–°
                cursor.execute("""
                    INSERT OR REPLACE INTO document_embeddings 
                    (document_id, chunk_index, chunk_text, start_pos, end_pos, 
                     embedding, embedding_model, metadata, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                """, (document_id, chunk.chunk_index, chunk.text, 
                      chunk.start_pos, chunk.end_pos, embedding_blob,
                      'BAAI/bge-large-zh-v1.5', metadata_json))
            
            conn.commit()
            logger.info(f"Stored {len(chunks)} embeddings for document {document_id} with hash {content_hash}")

    def get_document_hash(self, document_id: str) -> Optional[str]:
        """è·å–æ–‡æ¡£å†…å®¹å“ˆå¸Œå€¼"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT metadata FROM document_embeddings 
                WHERE document_id = ? 
                ORDER BY chunk_index 
                LIMIT 1
            """, (document_id,))
            
            row = cursor.fetchone()
            if row and row[0]:
                metadata = json.loads(row[0])
                return metadata.get('content_hash')
            
            return None
    
    def has_document_changed(self, document_id: str, content: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦å‘ç”Ÿå˜åŒ–"""
        # è®¡ç®—å½“å‰å†…å®¹å“ˆå¸Œ
        current_hash = hashlib.md5(content.encode()).hexdigest()
        
        # è·å–å­˜å‚¨çš„å“ˆå¸Œ
        stored_hash = self.get_document_hash(document_id)
        
        # å¦‚æœæ²¡æœ‰å­˜å‚¨çš„å“ˆå¸Œæˆ–å“ˆå¸Œä¸åŒï¼Œè¯´æ˜å†…å®¹å‘ç”Ÿäº†å˜åŒ–
        return stored_hash != current_hash
    
    def update_document_hash(self, document_id: str, content: str):
        """æ›´æ–°æ–‡æ¡£å†…å®¹å“ˆå¸Œå€¼"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰è¯¥æ–‡æ¡£çš„åµŒå…¥è®°å½•
            cursor.execute("""
                SELECT id FROM document_embeddings 
                WHERE document_id = ?
            """, (document_id,))
            
            for row in cursor.fetchall():
                embedding_id = row[0]
                
                # æ›´æ–°æ¯ä¸ªåµŒå…¥è®°å½•çš„å…ƒæ•°æ®
                cursor.execute("""
                    UPDATE document_embeddings 
                    SET metadata = json_set(
                        COALESCE(metadata, '{}'), 
                        '$.content_hash', 
                        ?
                    ),
                    updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (content_hash, embedding_id))
            
            conn.commit()
            logger.debug(f"Updated content hash for document {document_id}: {content_hash}")
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("DELETE FROM document_embeddings")
            cursor.execute("DELETE FROM search_history")
            conn.commit()
            logger.info("All vector data cleared")
    
    
    def _extract_keywords_with_ai(self, query_text: str) -> List[str]:
        """ä½¿ç”¨AIæå–å…³é”®è¯ï¼ˆå½“ä¼ ç»Ÿæ–¹æ³•å¤±è´¥æ—¶ï¼‰"""
        try:
            import time
            start_time = time.time()
            
            # è·å–AIé…ç½®
            ai_config = self._get_ai_config_for_keywords()
            if not ai_config:
                logger.warning("[AI_KEYWORDS] AIé…ç½®ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨AIæå–å…³é”®è¯")
                return []
            
            # æ„å»ºä¸“é—¨çš„å…³é”®è¯æå–æç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬å…³é”®è¯æå–å™¨ã€‚è¯·ä»ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ä¸­æå–2-4ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œç”¨äºæ–‡æ¡£æ£€ç´¢ã€‚

è¦æ±‚ï¼š
1. æå–çš„å…³é”®è¯å¿…é¡»æ˜¯æ–‡æœ¬ä¸­çš„æ ¸å¿ƒæ¦‚å¿µ
2. ä¼˜å…ˆæå–äººåã€åœ°åã€ç‰©å“åç­‰ä¸“æœ‰åè¯
3. å…³é”®è¯é•¿åº¦ä¸º2-4ä¸ªå­—ç¬¦
4. ç›´æ¥è¾“å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜

æ–‡æœ¬ï¼š{query_text}

å…³é”®è¯ï¼š"""

            # ä½¿ç”¨requestså‘é€è¯·æ±‚
            import requests
            import json
            
            headers = {
                "Authorization": f"Bearer {ai_config['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": ai_config.get('model', 'gpt-3.5-turbo'),
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 50,
                "temperature": 0.3
            }
            
            logger.info(f"[AI_KEYWORDS] å‘é€AIå…³é”®è¯æå–è¯·æ±‚...")
            
            response = requests.post(
                ai_config['api_url'],
                headers=headers,
                json=data,
                timeout=10.0  # 10ç§’è¶…æ—¶
            )
            
            request_time = time.time() - start_time
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    if 'choices' in result and len(result['choices']) > 0:
                        content = result['choices'][0]['message']['content'].strip()
                        
                        # è§£æAIè¿”å›çš„å…³é”®è¯
                        keywords = []
                        for keyword in content.split(','):
                            keyword = keyword.strip()
                            # æ¸…ç†å¯èƒ½çš„æ ¼å¼ç¬¦å·
                            keyword = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€""\'\s]+', '', keyword)
                            if len(keyword) >= 2 and len(keyword) <= 6:
                                keywords.append(keyword)
                        
                        keywords = keywords[:4]  # æœ€å¤š4ä¸ªå…³é”®è¯
                        
                        logger.info(f"[AI_KEYWORDS] AIå…³é”®è¯æå–æˆåŠŸï¼Œè€—æ—¶ {request_time:.2f}sï¼Œå…³é”®è¯: {keywords}")
                        return keywords
                    else:
                        logger.error(f"[AI_KEYWORDS] AIå“åº”æ ¼å¼é”™è¯¯: {result}")
                        return []
                        
                except (json.JSONDecodeError, KeyError) as e:
                    logger.error(f"[AI_KEYWORDS] è§£æAIå“åº”å¤±è´¥: {e}")
                    return []
            else:
                logger.error(f"[AI_KEYWORDS] AIè¯·æ±‚å¤±è´¥: {response.status_code}, {response.text}")
                return []
                
        except requests.exceptions.RequestException as e:
            logger.error(f"[AI_KEYWORDS] ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
            return []
        except Exception as e:
            logger.error(f"[AI_KEYWORDS] AIå…³é”®è¯æå–å¤±è´¥: {e}")
            import traceback
            logger.error(f"[AI_KEYWORDS] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return []
    
    def _get_ai_config_for_keywords(self) -> Optional[Dict[str, str]]:
        """è·å–ç”¨äºå…³é”®è¯æå–çš„AIé…ç½®"""
        try:
            # æ–¹æ³•1ï¼šä»å®‰å…¨å­˜å‚¨è·å–
            try:
                from .secure_key_manager import get_secure_key_manager
                key_manager = get_secure_key_manager()
                
                # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–æä¾›å•†ä¿¡æ¯
                import os
                provider = os.getenv('AI_PROVIDER', 'openai')
                api_key = key_manager.retrieve_api_key(provider)
                
                if api_key:
                    api_url = os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1/chat/completions')
                    model = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')
                    
                    logger.debug(f"[AI_KEYWORDS] ä»å®‰å…¨å­˜å‚¨è·å–AIé…ç½®: {provider}")
                    return {
                        'api_key': api_key,
                        'api_url': api_url,
                        'model': model
                    }
            except ImportError:
                logger.warning("å®‰å…¨å¯†é’¥ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åå¤‡æ–¹æ¡ˆ")
                
            # æ–¹æ³•2ï¼šå°è¯•ä»å…¨å±€é…ç½®è·å–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                # è¿™é‡Œå¯ä»¥å°è¯•å¯¼å…¥é…ç½®ç®¡ç†å™¨
                from .config import Config
                config = Config()
                ai_config = config.get_section('ai')
                
                if ai_config:
                    # ä»å®‰å…¨å­˜å‚¨è·å–API key
                    provider = ai_config.get('provider', 'openai')
                    try:
                        from .secure_key_manager import get_secure_key_manager
                        key_manager = get_secure_key_manager()
                        api_key = key_manager.retrieve_api_key(provider)
                    except ImportError:
                        api_key = None
                    
                    if api_key:
                        # æ ¹æ®providerç¡®å®šAPI URL
                        if provider == 'openai':
                            api_url = 'https://api.openai.com/v1/chat/completions'
                        elif provider == 'siliconflow':
                            api_url = 'https://api.siliconflow.cn/v1/chat/completions'
                        else:
                            api_url = ai_config.get('base_url', 'https://api.openai.com/v1/chat/completions')
                            if not api_url.endswith('/chat/completions'):
                                api_url = api_url.rstrip('/') + '/chat/completions'
                        
                        logger.debug(f"[AI_KEYWORDS] ä»é…ç½®æ–‡ä»¶è·å–AIé…ç½®: {provider}")
                        return {
                            'api_key': api_key,
                            'api_url': api_url,
                            'model': ai_config.get('model', 'gpt-3.5-turbo')
                        }
                    
            except ImportError:
                logger.debug("[AI_KEYWORDS] æ— æ³•å¯¼å…¥é…ç½®ç®¡ç†å™¨")
            except Exception as e:
                logger.debug(f"[AI_KEYWORDS] è·å–é…ç½®å¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤é…ç½®ï¼ˆä½œä¸ºæœ€åå›é€€ï¼‰
            logger.warning("[AI_KEYWORDS] æ— æ³•è·å–AIé…ç½®ï¼Œå…³é”®è¯æå–åŠŸèƒ½ä¸å¯ç”¨")
            return None
            
        except Exception as e:
            logger.error(f"[AI_KEYWORDS] è·å–AIé…ç½®æ—¶å‡ºé”™: {e}")
            return None

    def optimize(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("VACUUM")
            conn.execute("ANALYZE")