"""
ä¼˜åŒ–çš„å®ä½“æ£€æµ‹å™¨
ä¿®å¤æ—¶é—´è¡¨è¾¾å¼è¯¯è¯†åˆ«é—®é¢˜ï¼Œæ”¹è¿›ä¸­æ–‡å®ä½“è¯†åˆ«å‡†ç¡®æ€§
"""

import re
import logging
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from .reference_detector import ReferenceDetector, DetectedReference
from .codex_manager import CodexManager, CodexEntry, CodexEntryType
from .chinese_segmentation import get_segmenter, SegmentedWord, WordType

logger = logging.getLogger(__name__)


class EntityConfidence(Enum):
    """å®ä½“ç½®ä¿¡åº¦ç­‰çº§"""
    HIGH = "high"      # é«˜ç½®ä¿¡åº¦ (0.8-1.0)
    MEDIUM = "medium"  # ä¸­ç­‰ç½®ä¿¡åº¦ (0.6-0.8)
    LOW = "low"        # ä½ç½®ä¿¡åº¦ (0.4-0.6)
    VERY_LOW = "very_low"  # æä½ç½®ä¿¡åº¦ (0.0-0.4)


@dataclass
class OptimizedDetectedReference(DetectedReference):
    """ä¼˜åŒ–çš„æ£€æµ‹å¼•ç”¨ï¼ŒåŒ…å«ç½®ä¿¡åº¦ç­‰çº§å’Œè¿‡æ»¤åŸå› """
    confidence_level: EntityConfidence = EntityConfidence.MEDIUM
    filter_reasons: List[str] = None
    context_keywords: List[str] = None
    
    def __post_init__(self):
        if self.filter_reasons is None:
            self.filter_reasons = []
        if self.context_keywords is None:
            self.context_keywords = []


class OptimizedEntityDetector(ReferenceDetector):
    """ä¼˜åŒ–çš„å®ä½“æ£€æµ‹å™¨"""
    
    def __init__(self, codex_manager: CodexManager):
        super().__init__(codex_manager)
        
        # æ—¶é—´è¡¨è¾¾å¼è¿‡æ»¤æ¨¡å¼
        self.time_expression_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¬¡',  # ç¬¬Xæ¬¡
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¬¡äº†?',  # Xæ¬¡ã€Xæ¬¡äº†
            r'å†[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]*æ¬¡',  # å†Xæ¬¡
            r'åˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]*æ¬¡',  # åˆXæ¬¡
            r'[ä¸Šä¸‹]æ¬¡',  # ä¸Šæ¬¡ã€ä¸‹æ¬¡
            r'è¿™æ¬¡|é‚£æ¬¡|æ¯æ¬¡|æœ‰æ—¶|å¶å°”',  # æ—¶é—´å‰¯è¯
            r'[æ˜¨ä»Šæ˜]å¤©|[æ˜¨ä»Šæ˜]æ—¥',  # æ—¶é—´è¯
            r'[æ—©ä¸­æ™š]ä¸Š|[ä¸Šä¸‹]åˆ',  # æ—¶æ®µè¯
            r'åˆšæ‰|ç°åœ¨|é©¬ä¸Š|ç«‹åˆ»|éšå³',  # æ—¶é—´å‰¯è¯
            r'[å‰å]å¤©|[å‰å]æ—¥',  # ç›¸å¯¹æ—¶é—´
            r'è¿™ä¸ªæœˆ',  # æœˆä»½è¡¨è¾¾
            r'è¿™ä¸ªæœˆ.*æ¬¡',  # "è¿™ä¸ªæœˆç¬¬Xæ¬¡"æ¨¡å¼
            r'æœ¬æœˆ',  # æœˆä»½è¡¨è¾¾
            r'ä¸Šä¸ª?æœˆ',  # ä¸Šæœˆ
            r'ä¸‹ä¸ª?æœˆ',  # ä¸‹æœˆ
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ä¸ª?æœˆ',  # Xæœˆ
        ]
        
        # æ•°é‡è¡¨è¾¾å¼è¿‡æ»¤æ¨¡å¼
        self.quantity_patterns = [
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ä¸ªåªæ¡ä»¶å¼ ç‰‡å—]',  # æ•°é‡è¯
            r'[å‡ å¤šå°‘]ä¸ª?',  # ç–‘é—®æ•°é‡è¯
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[äººåä¸ª]',  # äººæ•°è¡¨è¾¾
        ]
        
        # å¦å®šè¯­å¢ƒæ¨¡å¼
        self.negative_context_patterns = [
            r'ä¸æ˜¯.{0,5}',  # ä¸æ˜¯X
            r'æ²¡æœ‰.{0,5}',  # æ²¡æœ‰X
            r'ä¸å«.{0,5}',  # ä¸å«X
            r'é.{0,3}',    # éX
            r'å¹¶é.{0,5}',  # å¹¶éX
            r'ä¸è®¤è¯†.{0,5}',  # ä¸è®¤è¯†X
            r'ä¸çŸ¥é“.{0,5}',  # ä¸çŸ¥é“X
        ]
        
        # å¸¸è§è¯¯è¯†åˆ«è¯æ±‡
        self.common_false_positives = {
            'ç¬¬ä¸‰æ¬¡', 'ç¬¬äºŒæ¬¡', 'ç¬¬ä¸€æ¬¡', 'è¿™æ¬¡', 'é‚£æ¬¡', 'ä¸Šæ¬¡', 'ä¸‹æ¬¡',
            'å‡ æ¬¡', 'å¤šæ¬¡', 'æœ‰æ—¶', 'å¶å°”', 'åˆšæ‰', 'ç°åœ¨', 'é©¬ä¸Š',
            'æ˜¨å¤©', 'ä»Šå¤©', 'æ˜å¤©', 'æ—©ä¸Š', 'ä¸­åˆ', 'æ™šä¸Š', 'ä¸‹åˆ',
            'ä¸€ä¸ª', 'ä¸¤ä¸ª', 'ä¸‰ä¸ª', 'å‡ ä¸ª', 'å¤šä¸ª', 'å°‘ä¸ª',
            'è¿™ä¸ªæœˆç¬¬ä¸‰æ¬¡äº†', 'è¿™ä¸ªæœˆç¬¬äºŒæ¬¡äº†', 'è¿™ä¸ªæœˆç¬¬ä¸€æ¬¡äº†',
            'ä¸€äºº', 'ä¸¤äºº', 'ä¸‰äºº', 'å‡ äºº', 'å¤šäºº',
        }
        
        # ç½®ä¿¡åº¦é˜ˆå€¼é…ç½®
        self.confidence_thresholds = {
            EntityConfidence.HIGH: 0.8,
            EntityConfidence.MEDIUM: 0.6,
            EntityConfidence.LOW: 0.4,
            EntityConfidence.VERY_LOW: 0.0
        }
        
        # æœ€ä½æ¥å—ç½®ä¿¡åº¦
        self.min_confidence_threshold = 0.6
        
        logger.info("OptimizedEntityDetector initialized with enhanced filtering")
    
    def detect_references(self, text: str, document_id: str = None) -> List[DetectedReference]:
        """
        ä¼˜åŒ–çš„å¼•ç”¨æ£€æµ‹ï¼ŒåŒ…å«æ—¶é—´è¡¨è¾¾å¼è¿‡æ»¤å’Œç½®ä¿¡åº¦è¯„ä¼°
        
        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬
            document_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ£€æµ‹åˆ°çš„å¼•ç”¨åˆ—è¡¨ï¼ˆå·²è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœï¼‰
        """
        if not text.strip():
            return []
        
        # 1. åŸºç¡€æ£€æµ‹
        raw_references = self._detect_raw_references(text)
        
        # 2. åº”ç”¨è¿‡æ»¤å™¨
        filtered_references = []
        for ref in raw_references:
            optimized_ref = self._apply_filters_and_scoring(ref, text)
            if optimized_ref and optimized_ref.confidence >= self.min_confidence_threshold:
                filtered_references.append(optimized_ref)
        
        # 3. å»é‡å’Œæ’åº
        filtered_references = self._deduplicate_references(filtered_references)
        filtered_references.sort(key=lambda x: x.start_position)
        
        # 4. æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        for ref in filtered_references:
            ref.context_before, ref.context_after = self._extract_context(
                text, ref.start_position, ref.end_position
            )
            # æå–ä¸Šä¸‹æ–‡å…³é”®è¯
            ref.context_keywords = self._extract_context_keywords(
                text, ref.start_position, ref.end_position
            )
        
        logger.debug(f"Optimized detection: {len(raw_references)} raw -> {len(filtered_references)} filtered")
        return filtered_references
    
    def _detect_raw_references(self, text: str) -> List[DetectedReference]:
        """æ‰§è¡ŒåŸºç¡€çš„å¼•ç”¨æ£€æµ‹"""
        references = []
        
        # ç›´æ¥å®ç°ç®€å•çš„æ–‡æœ¬åŒ¹é…
        for entry in self.codex_manager.get_all_entries():
            if not entry.track_references:
                continue
            
            # æ£€æµ‹æ ‡é¢˜åŒ¹é…
            title_matches = self._simple_text_match(text, entry.title, entry)
            references.extend(title_matches)
            
            # æ£€æµ‹åˆ«ååŒ¹é…
            for alias in entry.aliases:
                if alias.strip():
                    alias_matches = self._simple_text_match(text, alias.strip(), entry)
                    references.extend(alias_matches)
        
        return references
    
    def _simple_text_match(self, text: str, search_term: str, entry: CodexEntry) -> List[DetectedReference]:
        """ç®€å•çš„æ–‡æœ¬åŒ¹é…å®ç°"""
        if len(search_term) < 2:
            return []
        
        references = []
        import re
        
        # ä½¿ç”¨ç®€å•çš„æ­£åˆ™åŒ¹é…
        pattern = re.escape(search_term)
        
        try:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                reference = DetectedReference(
                    entry_id=entry.id,
                    entry_title=entry.title,
                    entry_type=entry.entry_type,
                    matched_text=match.group(),
                    start_position=match.start(),
                    end_position=match.end(),
                    confidence=1.0  # åŸºç¡€ç½®ä¿¡åº¦
                )
                references.append(reference)
        except re.error as e:
            logger.warning(f"Regex error for term '{search_term}': {e}")
        
        return references
    
    def _apply_filters_and_scoring(self, ref: DetectedReference, full_text: str) -> Optional[OptimizedDetectedReference]:
        """
        åº”ç”¨è¿‡æ»¤å™¨å’Œç½®ä¿¡åº¦è¯„åˆ†
        
        Args:
            ref: åŸå§‹æ£€æµ‹å¼•ç”¨
            full_text: å®Œæ•´æ–‡æœ¬
            
        Returns:
            ä¼˜åŒ–åçš„å¼•ç”¨æˆ–Noneï¼ˆå¦‚æœè¢«è¿‡æ»¤ï¼‰
        """
        matched_text = ref.matched_text
        filter_reasons = []
        
        # è¿‡æ»¤å™¨1: æ—¶é—´è¡¨è¾¾å¼è¿‡æ»¤
        if self._is_time_expression(matched_text):
            filter_reasons.append("æ—¶é—´è¡¨è¾¾å¼")
            logger.debug(f"Filtered time expression: {matched_text}")
            return None
        
        # è¿‡æ»¤å™¨2: æ•°é‡è¡¨è¾¾å¼è¿‡æ»¤
        if self._is_quantity_expression(matched_text):
            filter_reasons.append("æ•°é‡è¡¨è¾¾å¼")
            logger.debug(f"Filtered quantity expression: {matched_text}")
            return None
        
        # è¿‡æ»¤å™¨3: å¸¸è§è¯¯è¯†åˆ«è¯æ±‡è¿‡æ»¤
        if matched_text in self.common_false_positives:
            filter_reasons.append("å¸¸è§è¯¯è¯†åˆ«è¯æ±‡")
            logger.debug(f"Filtered common false positive: {matched_text}")
            return None
        
        # è¿‡æ»¤å™¨4: å¦å®šè¯­å¢ƒè¿‡æ»¤
        if self._is_in_negative_context(full_text, ref.start_position, matched_text):
            filter_reasons.append("å¦å®šè¯­å¢ƒ")
            logger.debug(f"Filtered negative context: {matched_text}")
            return None
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_optimized_confidence(ref, full_text)
        confidence_level = self._get_confidence_level(confidence)
        
        # åˆ›å»ºä¼˜åŒ–çš„å¼•ç”¨å¯¹è±¡
        optimized_ref = OptimizedDetectedReference(
            entry_id=ref.entry_id,
            entry_title=ref.entry_title,
            entry_type=ref.entry_type,
            matched_text=ref.matched_text,
            start_position=ref.start_position,
            end_position=ref.end_position,
            confidence=confidence,
            confidence_level=confidence_level,
            filter_reasons=filter_reasons
        )
        
        return optimized_ref
    
    def _is_time_expression(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæ—¶é—´è¡¨è¾¾å¼"""
        for pattern in self.time_expression_patterns:
            if re.fullmatch(pattern, text):
                return True
        return False
    
    def _is_quantity_expression(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæ•°é‡è¡¨è¾¾å¼"""
        for pattern in self.quantity_patterns:
            if re.fullmatch(pattern, text):
                return True
        return False
    
    def _is_in_negative_context(self, text: str, start_pos: int, matched_text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åœ¨å¦å®šè¯­å¢ƒä¸­"""
        # æ£€æŸ¥å‰é¢20ä¸ªå­—ç¬¦çš„ä¸Šä¸‹æ–‡
        context_start = max(0, start_pos - 20)
        context_before = text[context_start:start_pos]
        
        # æ£€æŸ¥åé¢10ä¸ªå­—ç¬¦çš„ä¸Šä¸‹æ–‡ï¼Œç”¨äºè¯†åˆ«è½¬æŠ˜
        context_end = min(len(text), start_pos + len(matched_text) + 10)
        context_after = text[start_pos + len(matched_text):context_end]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è½¬æŠ˜è¯ï¼Œå¦‚"è€Œæ˜¯"ã€"ä½†æ˜¯"ç­‰
        transition_words = ['è€Œæ˜¯', 'ä½†æ˜¯', 'ä¸è¿‡', 'ç„¶è€Œ', 'å¯æ˜¯']
        has_transition_before = any(word in context_before for word in transition_words)
        
        # å¦‚æœå‰é¢æœ‰è½¬æŠ˜è¯ï¼Œè¯´æ˜è¿™æ˜¯è‚¯å®šè¯­å¢ƒ
        if has_transition_before:
            return False
        
        # æ£€æŸ¥ç›´æ¥çš„å¦å®šæ¨¡å¼
        for pattern in self.negative_context_patterns:
            # æ„å»ºå®Œæ•´çš„å¦å®šæ¨¡å¼
            full_pattern = pattern + re.escape(matched_text)
            if re.search(full_pattern, context_before + matched_text):
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æœ‰è½¬æŠ˜ï¼Œå¦‚"ä¸æ˜¯å¼ ä¸‰ï¼Œè€Œæ˜¯æå››"
                full_context = context_before + matched_text + context_after
                if any(word in full_context for word in transition_words):
                    # å¦‚æœè½¬æŠ˜è¯åœ¨åŒ¹é…æ–‡æœ¬ä¹‹åï¼Œè¯´æ˜è¿™ä¸ªå®ä½“æ˜¯è¢«å¦å®šçš„
                    transition_after_match = any(word in context_after for word in transition_words)
                    if transition_after_match:
                        return True
                    else:
                        return False
                else:
                    return True
        
        return False
    
    def _calculate_optimized_confidence(self, ref: DetectedReference, full_text: str) -> float:
        """
        è®¡ç®—ä¼˜åŒ–çš„ç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            ref: æ£€æµ‹åˆ°çš„å¼•ç”¨
            full_text: å®Œæ•´æ–‡æœ¬
            
        Returns:
            float: ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)
        """
        base_confidence = 0.7  # åŸºç¡€ç½®ä¿¡åº¦
        factors = {}
        
        # å› å­1: åŒ¹é…é•¿åº¦åŠ æƒ
        text_length = len(ref.matched_text)
        if text_length >= 3:
            factors['length_bonus'] = 0.2
        elif text_length == 2:
            factors['length_bonus'] = 0.0
        else:
            factors['length_penalty'] = -0.3
        
        # å› å­2: è¾¹ç•Œè´¨é‡
        char_before = full_text[ref.start_position - 1] if ref.start_position > 0 else ""
        char_after = full_text[ref.end_position] if ref.end_position < len(full_text) else ""
        
        boundary_score = 0.0
        if self._is_delimiter(char_before) or not char_before:
            boundary_score += 0.1
        if self._is_delimiter(char_after) or not char_after:
            boundary_score += 0.1
        
        factors['boundary_quality'] = boundary_score
        
        # å› å­3: ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        context_score = self._calculate_context_relevance(ref, full_text)
        factors['context_relevance'] = context_score * 0.15
        
        # å› å­4: å®ä½“ç±»å‹åŒ¹é…åº¦
        type_score = self._calculate_entity_type_match(ref, full_text)
        factors['entity_type_match'] = type_score * 0.1
        
        # å› å­5: å…¨å±€æ¡ç›®åŠ æƒ
        entry = self.codex_manager.get_entry(ref.entry_id)
        if entry and entry.is_global:
            factors['global_entry_bonus'] = 0.1
        
        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        final_confidence = base_confidence + sum(factors.values())
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        return max(0.0, min(1.0, final_confidence))
    
    def _calculate_context_relevance(self, ref: DetectedReference, full_text: str) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§åˆ†æ•°"""
        try:
            # è·å–æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£
            context_size = 100
            start_pos = max(0, ref.start_position - context_size)
            end_pos = min(len(full_text), ref.end_position + context_size)
            context = full_text[start_pos:end_pos]
            
            score = 0.0
            
            # æ ¹æ®å®ä½“ç±»å‹æ£€æŸ¥ç›¸å…³å…³é”®è¯
            if ref.entry_type == CodexEntryType.CHARACTER:
                character_keywords = ['è¯´', 'é“', 'æƒ³', 'çœ‹', 'å¬', 'èµ°', 'æ¥', 'å»', 'ç¬‘', 'å“­', 'æ€’', 'å–œ']
                for keyword in character_keywords:
                    if keyword in context:
                        score += 0.1
                        if score >= 1.0:
                            break
            
            elif ref.entry_type == CodexEntryType.LOCATION:
                location_keywords = ['åœ¨', 'åˆ°', 'å»', 'æ¥', 'ä»', 'å‘', 'æœ', 'å¾€', 'å¤„', 'åœ°æ–¹']
                for keyword in location_keywords:
                    if keyword in context:
                        score += 0.1
                        if score >= 1.0:
                            break
            
            elif ref.entry_type == CodexEntryType.OBJECT:
                object_keywords = ['æ‹¿', 'ç”¨', 'æŒ', 'æ¡', 'æ”¾', 'å–', 'ç»™', 'é€’', 'æ‰”', 'ä¸¢']
                for keyword in object_keywords:
                    if keyword in context:
                        score += 0.1
                        if score >= 1.0:
                            break
            
            return min(score, 1.0)
        
        except Exception as e:
            logger.warning(f"Error calculating context relevance: {e}")
            return 0.0
    
    def _calculate_entity_type_match(self, ref: DetectedReference, full_text: str) -> float:
        """è®¡ç®—å®ä½“ç±»å‹åŒ¹é…åº¦"""
        matched_text = ref.matched_text
        
        # åŸºäºæ–‡æœ¬ç‰¹å¾åˆ¤æ–­å®ä½“ç±»å‹åŒ¹é…åº¦
        if ref.entry_type == CodexEntryType.CHARACTER:
            # è§’è‰²åé€šå¸¸æ˜¯2-4ä¸ªä¸­æ–‡å­—ç¬¦
            if 2 <= len(matched_text) <= 4 and all('\u4e00' <= c <= '\u9fff' for c in matched_text):
                return 1.0
            return 0.5
        
        elif ref.entry_type == CodexEntryType.LOCATION:
            # åœ°ç‚¹åå¯èƒ½åŒ…å«ç‰¹å®šåç¼€
            location_suffixes = ['åŸ', 'é•‡', 'æ‘', 'å›½', 'çœ', 'å¸‚', 'åŒº', 'å¿', 'å±±', 'æ²³', 'æ¹–', 'æµ·']
            if any(matched_text.endswith(suffix) for suffix in location_suffixes):
                return 1.0
            return 0.7
        
        elif ref.entry_type == CodexEntryType.OBJECT:
            # ç‰©å“åå¯èƒ½åŒ…å«ç‰¹å®šåç¼€
            object_suffixes = ['å‰‘', 'åˆ€', 'æª', 'ä¹¦', 'ç ', 'çŸ³', 'ä¸¹', 'è¯', 'ç¬¦', 'å°']
            if any(matched_text.endswith(suffix) for suffix in object_suffixes):
                return 1.0
            return 0.6
        
        return 0.5
    
    def _get_confidence_level(self, confidence: float) -> EntityConfidence:
        """æ ¹æ®ç½®ä¿¡åº¦åˆ†æ•°è·å–ç½®ä¿¡åº¦ç­‰çº§"""
        if confidence >= self.confidence_thresholds[EntityConfidence.HIGH]:
            return EntityConfidence.HIGH
        elif confidence >= self.confidence_thresholds[EntityConfidence.MEDIUM]:
            return EntityConfidence.MEDIUM
        elif confidence >= self.confidence_thresholds[EntityConfidence.LOW]:
            return EntityConfidence.LOW
        else:
            return EntityConfidence.VERY_LOW
    
    def _extract_context_keywords(self, text: str, start_pos: int, end_pos: int) -> List[str]:
        """æå–ä¸Šä¸‹æ–‡å…³é”®è¯ç”¨äºRAGæ£€ç´¢"""
        # æ‰©å¤§ä¸Šä¸‹æ–‡çª—å£ä»¥è·å–æ›´å¤šç›¸å…³ä¿¡æ¯
        context_size = 200  # å¢åŠ ä¸Šä¸‹æ–‡çª—å£å¤§å°
        context_start = max(0, start_pos - context_size)
        context_end = min(len(text), end_pos + context_size)
        
        # è·å–è§¦å‘ä½ç½®ä¹‹å‰çš„å†…å®¹ä½œä¸ºä¸»è¦ä¸Šä¸‹æ–‡
        before_context = text[context_start:start_pos]
        after_context = text[end_pos:context_end]
        
        # é‡ç‚¹å…³æ³¨è§¦å‘ä½ç½®ä¹‹å‰çš„å†…å®¹
        primary_context = before_context
        secondary_context = after_context
        
        keywords = []
        
        try:
            # ä½¿ç”¨ç®€å•çš„ä¸­æ–‡åˆ†è¯æå–å…³é”®è¯
            # æå–åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰
            import jieba
            import jieba.posseg as pseg
            logger.critical("ğŸ¯[JIEBA_DEBUG] optimized_entity_detectorä¸­jiebaå¯¼å…¥æˆåŠŸï¼Œå‡†å¤‡æå–ä¸Šä¸‹æ–‡å…³é”®è¯")
            
            # åˆ†æä¸»è¦ä¸Šä¸‹æ–‡ï¼ˆè§¦å‘ä½ç½®ä¹‹å‰ï¼‰
            primary_words = pseg.cut(primary_context)
            for word, flag in primary_words:
                # æ‰©å±•è¯æ€§æ ‡è®°ï¼ŒåŒ…å«æ›´å¤šæœ‰æ„ä¹‰çš„è¯æ±‡
                if len(word) >= 2 and flag in ['n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'ad', 'an', 't', 'i', 'l']:
                    keywords.append(word)
            
            # åˆ†ææ¬¡è¦ä¸Šä¸‹æ–‡ï¼ˆè§¦å‘ä½ç½®ä¹‹åï¼‰
            secondary_words = pseg.cut(secondary_context)
            for word, flag in secondary_words:
                # æ‰©å±•è¯æ€§æ ‡è®°ï¼ŒåŒ…å«æ›´å¤šæœ‰æ„ä¹‰çš„è¯æ±‡
                if len(word) >= 2 and flag in ['n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'ad', 'an', 't', 'i', 'l']:
                    keywords.append(word)
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            seen = set()
            unique_keywords = []
            for keyword in keywords:
                if keyword not in seen:
                    seen.add(keyword)
                    unique_keywords.append(keyword)
            
            # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œä¼˜å…ˆä¿ç•™å‰é¢çš„ï¼ˆæ›´æ¥è¿‘è§¦å‘ä½ç½®ï¼‰
            return unique_keywords[:10]
        
        except ImportError as e:
            logger.critical("âŒ[JIEBA_DEBUG] optimized_entity_detectorä¸­jiebaå¯¼å…¥å¤±è´¥: %s", e)
            logger.warning("jieba not available, using simple keyword extraction")
            # ç®€å•çš„å…³é”®è¯æå–ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            return self._simple_keyword_extraction(primary_context + secondary_context)
        except Exception as e:
            logger.warning(f"Error extracting context keywords: {e}")
            return []
    
    def _simple_keyword_extraction(self, context: str) -> List[str]:
        """ç®€å•çš„å…³é”®è¯æå–ï¼ˆä¸ä¾èµ–jiebaï¼‰"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯èƒ½çš„å…³é”®è¯
        keywords = []
        
        # æå–2-4ä¸ªä¸­æ–‡å­—ç¬¦çš„è¯æ±‡
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', context)
        
        # è¿‡æ»¤æ‰å¸¸è§çš„åœç”¨è¯
        stop_words = {'è¿™ä¸ª', 'é‚£ä¸ª', 'ä¸€ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'æ—¶å€™'}
        
        for word in chinese_words:
            if word not in stop_words and len(word) >= 2:
                keywords.append(word)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        return list(dict.fromkeys(keywords))[:8]
    
    def get_detection_statistics(self) -> Dict[str, any]:
        """è·å–ä¼˜åŒ–æ£€æµ‹å™¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        base_stats = super().get_detection_statistics()
        
        # æ·»åŠ ä¼˜åŒ–ç›¸å…³çš„ç»Ÿè®¡
        base_stats.update({
            'optimization_config': {
                'min_confidence_threshold': self.min_confidence_threshold,
                'time_expression_patterns': len(self.time_expression_patterns),
                'quantity_patterns': len(self.quantity_patterns),
                'negative_context_patterns': len(self.negative_context_patterns),
                'common_false_positives': len(self.common_false_positives)
            },
            'confidence_thresholds': {
                level.value: threshold 
                for level, threshold in self.confidence_thresholds.items()
            }
        })
        
        return base_stats
    
    def update_confidence_threshold(self, new_threshold: float):
        """æ›´æ–°æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼"""
        if 0.0 <= new_threshold <= 1.0:
            self.min_confidence_threshold = new_threshold
            logger.info(f"Updated minimum confidence threshold to {new_threshold}")
        else:
            logger.warning(f"Invalid confidence threshold: {new_threshold}")
    
    def add_false_positive_filter(self, word: str):
        """æ·»åŠ è¯¯è¯†åˆ«è¯æ±‡åˆ°è¿‡æ»¤åˆ—è¡¨"""
        if word and word not in self.common_false_positives:
            self.common_false_positives.add(word)
            logger.info(f"Added false positive filter: {word}")
    
    def remove_false_positive_filter(self, word: str):
        """ä»è¿‡æ»¤åˆ—è¡¨ä¸­ç§»é™¤è¯æ±‡"""
        if word in self.common_false_positives:
            self.common_false_positives.remove(word)
            logger.info(f"Removed false positive filter: {word}")